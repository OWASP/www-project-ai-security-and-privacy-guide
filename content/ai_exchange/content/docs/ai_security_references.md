---
title: AI Security References
weight: 7
---
## References of the OWASP AI Exchange
>Category: discussion  
>Permalink: https://owaspai.org/goto/references/

See the [Media page](/media) for several webinars and podcast by and about the AI Exchange.

## Overviews of AI Security Threats:
---
- [OWASP LLM top 10](https://genai.owasp.org/)
- [ENISA Cybersecurity threat landscape](https://www.enisa.europa.eu/publications/artificial-intelligence-cybersecurity-challenges)
- [ENISA ML threats and countermeasures 2021](https://www.enisa.europa.eu/publications/securing-machine-learning-algorithms)
- [MITRE ATLAS framework for AI threats](https://atlas.mitre.org/)
- [NIST threat taxonomy](https://csrc.nist.gov/publications/detail/white-paper/2023/03/08/adversarial-machine-learning-taxonomy-and-terminology/draft)
- [ETSI SAI](https://www.etsi.org/technologies/securing-artificial-intelligence)
- [Microsoft AI failure modes](https://docs.microsoft.com/en-us/security/failure-modes-in-machine-learning)
- [NIST](https://csrc.nist.gov/pubs/ai/100/2/e2023/final)
- [NISTIR 8269 - A Taxonomy and Terminology of Adversarial Machine Learning](https://csrc.nist.rip/external/nvlpubs.nist.gov/nistpubs/ir/2019/NIST.IR.8269-draft.pdf)
- [OWASP ML top 10](https://mltop10.info/)
- [BIML](https://berryvilleiml.com/taxonomy/)
- [PLOT4ai threat library](https://plot4.ai/library)
- [BSI AI recommendations including security aspects (Germany) - in English](https://www.bsi.bund.de/EN/Themen/Unternehmen-und-Organisationen/Informationen-und-Empfehlungen/Kuenstliche-Intelligenz/kuenstliche-intelligenz_node.html#doc916902bodyText8)
- [NCSC UK / CISA Joint Guidelines](https://www.ncsc.gov.uk/collection/guidelines-secure-ai-system-development) - see [its mapping with the AI Exchange](/goto/jointguidelines/)

## Overviews of AI Security/Privacy Incidents:
---
- [AVID AI Vulnerability database](https://avidml.org/)
- [Sightline - AI/ML Supply Chain Vulnerability Database](https://sightline.protectai.com/)
- [OECD AI Incidents Monitor (AIM)](https://oecd.ai/en/incidents)
- [AI Incident Database](https://incidentdatabase.ai/)
- [AI Exploits by ProtectAI](https://github.com/protectai/ai-exploits)

## Misc.:
---
- [ENISA AI security standard discussion](https://www.enisa.europa.eu/publications/cybersecurity-of-ai-and-standardisation)
- [ENISA's multilayer AI security framework](https://www.enisa.europa.eu/publications/multilayer-framework-for-good-cybersecurity-practices-for-ai)
- [Alan Turing institute's AI standards hub](https://aistandardshub.org)
- [Microsoft/MITRE tooling for ML teams](https://www.mitre.org/news-insights/news-release/microsoft-and-mitre-create-tool-help-security-teams-prepare-attacks?sf175190906=1)
- [Google's Secure AI Framework](https://blog.google/technology/safety-security/introducing-googles-secure-ai-framework/)
- [NIST AI Risk Management Framework 1.0](https://doi.org/10.6028/NIST.AI.100-1)
- [ISO/IEC 20547-4 Big data security](https://www.iso.org/standard/71278.html)
- [IEEE 2813 Big Data Business Security Risk Assessment](https://standards.ieee.org/ieee/2813/7535/)
- [Awesome MLSecOps references](https://github.com/RiccardoBiosas/awesome-MLSecOps)
- [OffSec ML Playbook](https://wiki.offsecml.com/)
- [MIT AI Risk Repository](https://airisk.mit.edu/)
- [Failure Modes in Machine Learning by Microsoft](https://learn.microsoft.com/en-us/security/engineering/failure-modes-in-machine-learning)

## Learning and Training:
---
| Category                   | Title                                        | Description                                                                                       | Provider       | Content Type     | Level       | Cost                             | Link                                                                                         |
|---------------------------|----------------------------------------------|---------------------------------------------------------------------------------------------------|----------------|------------------|-------------|----------------------------------|----------------------------------------------------------------------------------------------|
| **Courses and Labs**               | **AI Security Fundamentals**                 | Learn the basic concepts of AI security, including security controls and testing procedures.      | Microsoft       | Course           | Beginner    | Free                             | [AI Security Fundamentals](https://learn.microsoft.com/en-us/training/paths/ai-security-fundamentals/) |
|                           | **Red Teaming LLM Applications**            | Explore fundamental vulnerabilities in LLM applications with hands-on lab practice.               | Giskard         | Course + Lab     | Beginner    | Free                             | [Red Teaming LLM Applications](https://www.deeplearning.ai/short-courses/red-teaming-llm-applications/) |
|                           | **Exploring Adversarial Machine Learning**  | Designed for data scientists and security professionals to learn how to attack realistic ML systems.| NVIDIA          | Course + Lab     | Intermediate | Paid                             | [Exploring Adversarial Machine Learning](https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+S-DS-03+V1) |
|                           | **OWASP LLM Vulnerabilities**                | Essentials of securing Large Language Models (LLMs), covering basic to advanced security practices.| Checkmarx | Interactive Lab | Beginner    | Free with OWASP Membership      | [OWASP LLM Vulnerabilities](https://owasp.codebashing.com/app/course?courseUuid=d0e55509-bff3-4860-8d0e-141a59ef152b) |
|                           | **OWASP TOP 10 for LLM**                    | Scenario-based LLM security vulnerabilities and their mitigation strategies.                      | Security Compass          | Interactive Lab   | Beginner    | Free                             | [OWASP TOP 10 for LLM](https://application.security/free/llm) |
|                           | **Web LLM Attacks**                         | Hands-on lab to practice exploiting LLM vulnerabilities.                                         | Portswigger     | Lab              | Beginner    | Free                             | [Web LLM Attacks](https://portswigger.net/web-security/llm-attacks) |
|                           | **Path: AI Red Teamer**                    | Covers OWASP ML/LLM Top 10 and attacking ML-based systems.                      | HackTheBox Academy          | Course + Lab   | Beginner    | Paid                           | [HTB AI Red Teamer](https://academy.hackthebox.com/) |
|                           | **Path: Artificial Intelligence and Machine Learning**                         | Hands-on lab to practice AI/ML vulnerabilities exploitation.                                         | HackTheBox Enterprise     | Dedicated Lab         | Beginner, Intermediate    | Enterprise Plan                   | [HTB AI/ML Lab](https://enterprise.hackthebox.com/) |
| **CTF Practices**      | **AI Capture The Flag**                     | A series of AI-themed challenges ranging from easy to hard, hosted by DEFCON AI Village.         | Crucible / AIV        | CTF              | Beginner, Intermediate | Free                             | [AI Capture The Flag](https://crucible.dreadnode.io/) |
|                           | **IEEE SaTML CTF 2024**                     | A Capture-the-Flag competition focused on Large Language Models.                                 | IEEE            | CTF              | Beginner, Intermediate | Free                             | [IEEE SaTML CTF 2024](https://ctf.spylab.ai/) |
|                           | **Gandalf Prompt CTF**                      | A gamified challenge focusing on prompt injection techniques.                                     | Lakera          | CTF              | Beginner    | Free                             | [Gandalf Prompt CTF](https://gandalf.lakera.ai/) |
|                           | **HackAPrompt**                             | A prompt injection playground for participants of the HackAPrompt competition.                   | AiCrowd         | CTF               | Beginner    | Free                             | [HackAPrompt](https://huggingface.co/spaces/hackaprompt/playground) |
|                           | **Prompt Airlines**                         | Manipulate AI chatbot via prompt injection to score a free airline ticket.                       | WiZ             | CTF  | Beginner    | Free                             | [PromptAirlines](https://promptairlines.com/) |
|                           | **AI CTF**                                  | AI/ML themed challenges to be solved over a 36-hour period.                                       | PHDay           | CTF              | Beginner, Intermediate | Free                             | [AI CTF](https://aictf.phdays.fun/) |
|                           | **Prompt Injection Lab**                    | An immersive lab focused on gamified AI prompt injection challenges.                              | ImmersiveLabs    | CTF              | Beginner    | Free                             | [Prompt Injection Lab](https://prompting.ai.immersivelabs.com/) |
|                           | **Doublespeak**                             | A text-based AI escape game designed to practice LLM vulnerabilities.                             | Forces Unseen    | CTF              | Beginner    | Free                             | [Doublespeak](https://doublespeak.chat/#/) |
|                           | **MyLLMBank**                      | Prompt injection challenges against LLM chat agents that use ReAct to call tools.                                     | WithSecure          | CTF              | Beginner    | Free                             | [MyLLLBank](https://myllmbank.com/)|
|                           | **MyLLMDoctor**                      | Advanced challenge focusing on multi-chain prompt injection.                                     | WithSecure          | CTF              | Intermediate    | Free                             | [MyLLMDoctor](https://myllmdoc.com/) |
|                           | **Damn vulnerable LLM agent**                      | Focuses on Thought/Action/Observation injection                                     | WithSecure          | CTF              | Intermediate    | Free                             | [Damn vulnerable LLM agent](https://github.com/WithSecureLabs/damn-vulnerable-llm-agent) |
| **Talks**                  | **AI is just software, what could possible go wrong w/ Rob van der Veer** | The talk explores the dual nature of AI as both a powerful tool and a potential security risk, emphasizing the importance of secure AI development and oversight. | OWASP Lisbon Global AppSec 2024 | Conference | N/A | Free | [YouTube](https://www.youtube.com/watch?v=43cv4f--UU4)  |
|                          | **Lessons Learned from Building & Defending LLM Applications** | Andra Lezza and Javan Rasokat discuss lessons learned in AI security, focusing on vulnerabilities in LLM applications. | DEF CON 32 | Conference | N/A | Free | [YouTube](https://www.youtube.com/watch?v=2-C7xSJ9rhI)  |
|                          | **Practical LLM Security: Takeaways From a Year in the Trenches** | NVIDIAâ€™s AI Red Team shares insights on securing LLM integrations, focusing on identifying risks, common attacks, and effective mitigation strategies. | Black Hat USA 2024 | Conference | N/A | Free | [YouTube](https://www.youtube.com/watch?v=Rhpqiunpu0c)  |
|                          | **Hacking generative AI with PyRIT** | Rajasekar from Microsoft AI Red Team presents PyRIT, a tool for identifying vulnerabilities in generative AI systems, emphasizing the importance of safety and security. | Black Hat USA 2024 | Walkthrough | N/A | Free | [YouTube](https://www.youtube.com/watch?v=M_H8ulTMAe4)  |
