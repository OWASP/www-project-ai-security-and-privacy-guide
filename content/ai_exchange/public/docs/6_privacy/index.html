<!doctype html><html lang=en><head><meta property="og:title" content="6. AI privacy – AI Exchange"><meta property="og:description" content="Comprehensive guidance and alignment on how to protect AI against security threats - by professionals, for professionals."><meta property="og:type" content="article"><meta property="og:url" content="https://owaspai.org/docs/6_privacy/"><meta property="og:image" content="https://owaspai.org/images/aix-og-logo.jpg"><meta property="article:section" content="docs"><meta charset=utf-8><script src=https://cdn.tailwindcss.com></script><meta name=viewport content="width=device-width,initial-scale=1"><title>6. AI privacy | AI Exchange</title><meta name=description content><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100..900;1,100..900&display=swap" rel=stylesheet><link rel=stylesheet href=/css/custom-style.css><script src=/js/script.js></script>
<link href=https://unpkg.com/aos@2.3.1/dist/aos.css rel=stylesheet><script src=https://unpkg.com/aos@2.3.1/dist/aos.js></script>
<script>AOS.init({delay:0,disable:"phone"})</script></head><body><header class=main-header><div class=header-background></div><div class="container header-container"><div class=logo-section><a href=/ class=logo-link><img src=/images/Owasp-AI-Exchange-Logo.png alt="AI Exchange" class=site-logo width=Auto height=48></a></div><nav class=main-navigation><ul class=nav-list><li class=nav-item><a href=/ class=nav-link>Home</a></li><li class=nav-item><a href=/docs/ai_security_overview/ class=nav-link>Overview</a></li><li class=nav-item><a href=/media/ class=nav-link>Media</a></li><li class=nav-item><a href=/contribute/ class=nav-link>Contribute</a></li><li class=nav-item><a href=/connect/ class=nav-link>Connect</a></li><li class=nav-item><a href=/sponsor/ class=nav-link>Sponsor</a></li></ul></nav><div class=header-actions><div class=social-icons><a href=https://github.com/OWASP/www-project-ai-security-and-privacy-guide class=social-link title=GitHub><svg class="social-icon" viewBox="0 0 24 24" fill="currentcolor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23A11.509 11.509.0 0112 5.803c1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg></a><a href=https://github.com/OWASP/www-project-ai-security-and-privacy-guide class=social-link title=Slack><svg class="social-icon" viewBox="0 0 24 24" fill="currentcolor"><path d="M5.042 15.165a2.528 2.528.0 01-2.52 2.523A2.528 2.528.0 010 15.165a2.527 2.527.0 012.522-2.52h2.52v2.52zm1.271.0a2.527 2.527.0 012.521-2.52 2.527 2.527.0 012.521 2.52v6.313A2.528 2.528.0 018.834 24a2.528 2.528.0 01-2.521-2.522v-6.313zM8.834 5.042a2.528 2.528.0 01-2.521-2.52A2.528 2.528.0 018.834.0a2.528 2.528.0 012.521 2.522v2.52H8.834zm0 1.271a2.528 2.528.0 012.521 2.521 2.528 2.528.0 01-2.521 2.521H2.522A2.528 2.528.0 010 8.834a2.528 2.528.0 012.522-2.521h6.312zM18.956 8.834a2.528 2.528.0 012.522-2.521A2.528 2.528.0 0124 8.834a2.528 2.528.0 01-2.522 2.521h-2.522V8.834zm-1.268.0a2.528 2.528.0 01-2.523 2.521 2.527 2.527.0 01-2.52-2.521V2.522A2.527 2.527.0 0115.165.0a2.528 2.528.0 012.523 2.522v6.312zM15.165 18.956a2.528 2.528.0 012.523 2.522A2.528 2.528.0 0115.165 24a2.527 2.527.0 01-2.52-2.522v-2.522h2.52zm0-1.268a2.527 2.527.0 01-2.52-2.523 2.526 2.526.0 012.52-2.52h6.313A2.527 2.527.0 0124 15.165a2.528 2.528.0 01-2.522 2.523h-6.313z"/></svg></a><a href=https://github.com/OWASP/www-project-ai-security-and-privacy-guide class=social-link title=LinkedIn><svg class="social-icon" viewBox="0 0 24 24" fill="currentcolor"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853.0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601.0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144.0-2.063-.926-2.063-2.065.0-1.138.92-2.063 2.063-2.063 1.14.0 2.064.925 2.064 2.063.0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225.0H1.771C.792.0.0.774.0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2.0 22.222.0h.003z"/></svg></a><a href=https://github.com/OWASP/www-project-ai-security-and-privacy-guide class=social-link title=Youtube><svg class="social-icon" viewBox="0 0 24 24" fill="currentcolor"><path d="M23.498 6.186A3.016 3.016.0 0021.376 4.05C19.505 3.545 12 3.545 12 3.545s-7.505.0-9.377.505A3.017 3.017.0 00.502 6.186C0 8.07.0 12 0 12s0 3.93.502 5.814a3.016 3.016.0 002.122 2.136c1.871.505 9.376.505 9.376.505s7.505.0 9.377-.505a3.015 3.015.0 002.122-2.136C24 15.93 24 12 24 12s0-3.93-.502-5.814zM9.545 15.568V8.432L15.818 12l-6.273 3.568z"/></svg></a></div><div class=search-icon><svg class="search-svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2"><circle cx="11" cy="11" r="8"/><path d="m21 21-4.35-4.35"/></svg></div></div></div></header><main id=main><section class="intro-banner text-white py-12 mb-0 bg-cover bg-center bg-no-repeat" style=background-image:url(/images/overview-hero.png)><div class="container text-left bg-opacity-50 p-8 rounded-lg"><h1 class="text-[46px] md:text-5xl font-bold mb-4">6. AI privacy</h1><p class="text-lg text-gray-300">Engage with the OWASP AI team through various platforms.</p><button class="inline-block bg-[#4CAF50] hover:bg-emerald-700 text-white text-xl font-medium py-3 px-10 rounded-lg mt-[22px]">
Let's connect</button></div></section><div class=docs-layout><div class=docs-sidebar-column><aside class="docs-sidebar p-4"><div class=sidebar-container><nav class=sidebar-nav><ul class="flex flex-col"><li><a href=/docs/ai_security_overview/ class=sidebar-nav-link><span>0. AI Security Overview</span></a></li><li><a href=/docs/1_general_controls/ class=sidebar-nav-link><span>1. General controls</span></a></li><li><a href=/docs/2_threats_through_use/ class=sidebar-nav-link><span>2. Threats through use</span></a></li><li><a href=/docs/3_development_time_threats/ class=sidebar-nav-link><span>3. Development-time threats</span></a></li><li><a href=/docs/4_runtime_application_security_threats/ class=sidebar-nav-link><span>4. Runtime application security threats</span></a></li><li><a href=/docs/5_testing/ class=sidebar-nav-link><span>5. AI security testing</span></a></li><li><a href=/docs/6_privacy/ class="sidebar-nav-link active"><span>6. AI privacy</span></a></li><li><a href=/docs/ai_security_references/ class=sidebar-nav-link><span>AI Security References</span></a></li><li><a href=/docs/ class=sidebar-nav-link><span>Index</span></a></li></ul></nav></div></aside><div class=sidebar-more><h3 class=sidebar-title>More</h3><ul class="flex flex-col"><li><a href=/ class=sidebar-menu-item>Home</a></li><li><a href=/connect/ class=sidebar-menu-item>Connect with us</a></li><li><a href=/contribute/ class=sidebar-menu-item>Contribute</a></li><li><a href=/media/ class=sidebar-menu-item>Media</a></li><li><a href=/meetings/ class=sidebar-menu-item>Meetings</a></li><li><a href=https://forms.gle/XwEEK52y4iZQChuJ6 class=sidebar-menu-item>Register</a></li><li><a href=/sponsor/ class=sidebar-menu-item>Sponsor</a></li></ul></div></div><main class=docs-main><div class=docs-content><nav class=breadcrumbs><a href=/>Home</a>
<span class=breadcrumb-separator>></span>
<span class=current-page>6. AI privacy</span></nav><h1 class=docs-title>6. AI privacy</h1><div class=docs-body><blockquote><p>Category: discussion<br>Permalink: <a href=https://owaspai.org/goto/aiprivacy/ target=_blank rel=noopener>https://owaspai.org/goto/aiprivacy/</a></p></blockquote><h2>Introduction<span class="absolute -mt-20" id=introduction></span>
<a href=#introduction class=subheading-anchor aria-label="Permalink for this section"></a></h2><p>This section of the AI Exchange covers how privacy principles apply to AI systems. The rest of the AI Exchange covers the security of AI systems, including the protection of personal data, but there is more to privacy than just that - which is the topic of this section.</p><h3>Privacy concerns of AI systems<span class="absolute -mt-20" id=privacy-concerns-of-ai-systems></span>
<a href=#privacy-concerns-of-ai-systems class=subheading-anchor aria-label="Permalink for this section"></a></h3><p>Just like any system that processes data, AI systems can have privacy risks. There are specific privacy concerns associated with AI:</p><ul><li>AI systems are data-intensive and typically present additional risks regarding data collection and retention. Personal data may be collected from various sources, each subject to different levels of <strong>sensitivity and regulatory constraints</strong>. Legislation often requires a <strong>legal basis and/or consent</strong> for the collection and use of personal data, and specifies <strong>rights to individuals</strong> to correct, request, and remove their own data.</li><li><strong>Protecting training data</strong> is a challenge, especially because it typically needs to be retained for long periods - as many models need to be retrained. Often, the actual identities of people involved are irrelevant for the model, but privacy risks still remain even if identity data is removed because it might be possible to deduce individual identities from the remaining data. This is where differential privacy becomes crucial: by altering the data to make it sufficiently unrecognizable, it ensures individual privacy while still allowing for valuable insights to be derived from the data. Alteration can be achieved, for example, by adding noise or using aggregation techniques.</li><li>An additional complication in the protection of training data is that the <strong>training data is accessible in the engineering environment</strong>, which therefore needs more protection than it usually does - since conventional systems normally don&rsquo;t have personal data available to technical teams.</li><li>The nature of machine learning allows for certain <strong>unique strategies</strong> to improve privacy, such as federated learning: splitting up the training set in different separated systems - typically aligning with separated data collection.</li><li>AI systems <strong>make decisions</strong> and if these decisions are about people they may be discriminating regarding certain protected attributes (e.g. gender, race), plus the decisions may result in actions that invade privacy, which may be an ethical or legal concern. Furthermore, legislation may prohibit some types of decisions and sets rules regarding transparency about how these decisions are made, and about how individuals have the right to object.</li><li>Last but not least: AI models suffer from <strong>model attack risks</strong> that allow attackers to extract training data from the model, e.g. model inversion, membership inference, and disclosing sensitive data in large language models</li></ul><h3>Privacy = personal data protection + respect for further individual rights<span class="absolute -mt-20" id=privacy--personal-data-protection--respect-for-further-individual-rights></span>
<a href=#privacy--personal-data-protection--respect-for-further-individual-rights class=subheading-anchor aria-label="Permalink for this section"></a></h3><p>AI Privacy can be divided into two parts:</p><ol><li>The threats to AI security and their controls (see the other sections of the AI Exchange), including:</li></ol><ul><li>Confidentiality and integrity protection of personal data in train/test data, model input or output - which consists of:<ul><li>&lsquo;Conventional&rsquo; security of personal data in transit and in rest</li><li>Protecting against model attacks that try to retrieve personal data (e.g. model inversion)</li><li>Personal data minimization / differential privacy, including minimized retention</li></ul></li><li>Integrity protection of the model behaviour if that behaviour can hurt privacy of individuals. This happens for example when individuals are unlawfully discriminated or when the model output leads to actions that invade privacy (e.g. undergoing a fraud investigation).</li></ul><ol start=2><li>Threats and controls that are not about security, but about further rights of the individual, as covered by privacy regulations such as the GDPR, including use limitation, consent, fairness, transparency, data accuracy, right of correction/objection/erasure/request.</li></ol><h3>Legislation<span class="absolute -mt-20" id=legislation></span>
<a href=#legislation class=subheading-anchor aria-label="Permalink for this section"></a></h3><p>Privacy principles and requirements come from different legislations (e.g. GDPR, LGPD, PIPEDA, etc.) and privacy standards (e.g. ISO 31700, ISO 29100, ISO 27701, FIPS, NIST Privacy Framework, etc.). This guideline does not guarantee compliance with privacy legislation and it is also not a guide on privacy engineering of systems in general. For that purpose, please consider work from <a href=https://www.enisa.europa.eu/publications/data-protection-engineering target=_blank rel=noopener>ENISA</a>, <a href=https://nvlpubs.nist.gov/nistpubs/ir/2017/NIST.IR.8062.pdf target=_blank rel=noopener>NIST</a>, <a href=https://github.com/mplspunk/awesome-privacy-engineering target=_blank rel=noopener>mplsplunk</a>, <a href=https://owasp.org/www-project-top-10-privacy-risks/ target=_blank rel=noopener>OWASP</a> and <a href=https://www.opencre.org/cre/362-550 target=_blank rel=noopener>OpenCRE</a>. The general principle for engineers is to regard personal data as &lsquo;radioactive gold&rsquo;. It&rsquo;s valuable, but it&rsquo;s also something to minimize, carefully store, carefully handle, limit its usage, limit sharing, keep track of where it is, etc.</p><h3>Assessments<span class="absolute -mt-20" id=assessments></span>
<a href=#assessments class=subheading-anchor aria-label="Permalink for this section"></a></h3><p>Organizations often conduct Privacy Impact Assessments (PIAs) on systems to identify and manage privacy risks (also referred to as Data Protection Impact Assessments). This is a good idea for AI systems as well. It evaluates data flows, use cases, and AI behaviors against applicable privacy laws and ethical standards. This proactive assessment guides the implementation of privacy controls and helps embed privacy by design principles, ensuring privacy risks are minimized from the outset. Do note that PIAs are not per se specialized in AI systems and may overlook typical AI risks regarding:</p><ul><li>AI input attacks with privacy risks, such as Model inversion, membership inference, or sensitive data output from model.</li><li>Bias and fairness risks (systematic discrimination from training data).</li><li>Ongoing learning or retraining (new accuracy and bias risks can appear after deployment).</li><li>Explainability and accountability gaps (harder to trace decisions back).</li></ul><p>There are dedicated AI impact assessment methods available, such as:</p><ul><li><a href=https://ecp.nl/publicatie/artificial-intelligence-impact-assessment-english-version/ target=_blank rel=noopener>AI impact assessment from the Netherlands</a></li><li><a href=https://www.gov.uk/ai-assurance-techniques target=_blank rel=noopener>UK government overview of assessment techniques</a></li></ul><h2>1. Use Limitation and Purpose Specification<span class="absolute -mt-20" id=1-use-limitation-and-purpose-specification></span>
<a href=#1-use-limitation-and-purpose-specification class=subheading-anchor aria-label="Permalink for this section"></a></h2><p>Essentially, you should not simply use data collected for one purpose (e.g. safety or security) as a training dataset to train your model for other purposes (e.g. profiling, personalized marketing, etc.) For example, if you collect phone numbers and other identifiers as part of your MFA flow (to improve security ), that doesn&rsquo;t mean you can also use it for user targeting and other unrelated purposes. Similarly, you may need to collect sensitive data under KYC requirements, but such data should not be used for ML models used for business analytics without proper controls.</p><p>Some privacy laws require a lawful basis (or bases if used for more than one purpose) for processing personal data (See GDPR&rsquo;s Art 6 and 9).
Here is a link with certain restrictions on the purpose of an AI application, like for example the <a href=https://artificialintelligenceact.eu/article/5 target=_blank rel=noopener>prohibited practices in the European AI Act</a> such as using machine learning for individual criminal profiling. Some practices are regarded as too risky when it comes to potential harm and unfairness towards individuals and society.</p><p>Note that a use case may not even involve personal data, but can still be potentially harmful or unfair to individuals. For example: an algorithm that decides who may join the army, based on the amount of weight a person can lift and how fast the person can run. This data cannot be used to reidentify individuals (with some exceptions), but still the use case may be unrightfully unfair towards gender (if the algorithm for example is based on an unfair training set).</p><p>In practical terms, you should reduce access to sensitive data and create anonymized copies for incompatible purposes (e.g. analytics). You should also document a purpose/lawful basis before collecting the data and communicate that purpose to the user in an appropriate way.</p><p>New techniques that enable use limitation include:</p><ul><li>data enclaves: store pooled personal data in restricted secure environments</li><li>federated learning: decentralize ML by removing the need to pool data into a single location. Instead, the model is trained in multiple iterations at different sites.</li></ul><h2>2. Fairness<span class="absolute -mt-20" id=2-fairness></span>
<a href=#2-fairness class=subheading-anchor aria-label="Permalink for this section"></a></h2><p>Fairness means handling personal data in a way individuals expect and not using it in ways that lead to unjustified adverse effects. The algorithm should not behave in a discriminating way. (See also <a href=https://iapp.org/news/a/what-is-the-role-of-privacy-professionals-in-preventing-discrimination-and-ensuring-equal-treatment/ target=_blank rel=noopener>this article</a>). Furthermore: accuracy issues of a model becomes a privacy problem if the model output leads to actions that invade privacy (e.g. undergoing fraud investigation). Accuracy issues can be caused by a complex problem, insufficient data, mistakes in data and model engineering, and manipulation by attackers. The latter example shows that there can be a relation between model security and privacy.</p><p>GDPR&rsquo;s Article 5 refers to &ldquo;fair processing&rdquo; and EDPS&rsquo; <a href=https://edpb.europa.eu/sites/default/files/files/file1/edpb_guidelines_201904_dataprotection_by_design_and_by_default_v2.0_en.pdf target=_blank rel=noopener>guideline</a> defines fairness as the prevention of &ldquo;unjustifiably detrimental, unlawfully discriminatory, unexpected or misleading&rdquo; processing of personal data. GDPR does not specify how fairness can be measured, but the EDPS recommends the right to information (transparency), the right to intervene (access, erasure, data portability, rectify), and the right to limit the processing (right not to be subject to automated decision-making and non-discrimination) as measures and safeguards to implement the principle of fairness.</p><p>In the <a href=http://fairware.cs.umass.edu/papers/Verma.pdf target=_blank rel=noopener>literature</a>, there are different fairness metrics that you can use. These range from group fairness, false positive error rate, unawareness, and counterfactual fairness. There is no industry standard yet on which metric to use, but you should assess fairness especially if your algorithm is making significant decisions about the individuals (e.g. banning access to the platform, financial implications, denial of services/opportunities, etc.). There are also efforts to test algorithms using different metrics. For example, NIST&rsquo;s <a href=https://pages.nist.gov/frvt/html/frvt11.html target=_blank rel=noopener>FRVT project</a> tests different face recognition algorithms on fairness using different metrics.</p><p>The elephant in the room for fairness across groups (protected attributes) is that in situations a model is more accurate if it DOES discriminate protected attributes. Certain groups have in practice a lower success rate in areas because of all kinds of societal aspects rooted in culture and history. We want to get rid of that. Some of these aspects can be regarded as institutional discrimination. Others have more practical background, like for example that for language reasons we see that new immigrants statistically tend to be hindered in getting higher education.
Therefore, if we want to be completely fair across groups, we need to accept that in many cases this will be balancing accuracy with discrimination. In the case that sufficient accuracy cannot be attained while staying within discrimination boundaries, there is no other option than to abandon the algorithm idea. For fraud detection cases, this could for example mean that transactions need to be selected randomly instead of by using an algorithm.</p><p>A machine learning use case may have unsolvable bias issues, that are critical to recognize before you even start. Before you do any data analysis, you need to think if any of the key data elements involved have a skewed representation of protected groups (e.g. more men than women for certain types of education). I mean, not skewed in your training data, but in the real world. If so, bias is probably impossible to avoid - unless you can correct for the protected attributes. If you don&rsquo;t have those attributes (e.g. racial data) or proxies, there is no way. Then you have a dilemma between the benefit of an accurate model and a certain level of discrimination. This dilemma can be decided on before you even start, and save you a lot of trouble.</p><p>Even with a diverse team, with an equally distributed dataset, and without any historical bias, your AI may still discriminate. And there may be nothing you can do about it.<br>For example: take a dataset of students with two variables: study program and score on a math test. The goal is to let the model select students good at math for a special math program. Let&rsquo;s say that the study program &lsquo;computer science&rsquo; has the best scoring students. And let&rsquo;s say that much more males then females are studying computer science. The result is that the model will select more males than females. Without having gender data in the dataset, this bias is impossible to counter.</p><h2>3. Data Minimization and Storage Limitation<span class="absolute -mt-20" id=3-data-minimization-and-storage-limitation></span>
<a href=#3-data-minimization-and-storage-limitation class=subheading-anchor aria-label="Permalink for this section"></a></h2><p>This principle requires that you should minimize the amount, granularity and storage duration of personal information in your training dataset. To make it more concrete:</p><ul><li>Do not collect or copy unnecessary attributes to your dataset if this is irrelevant for your purpose</li><li>Anonymize the data where possible. Please note that this is not as trivial as &ldquo;removing PII&rdquo;. See <a href=https://ec.europa.eu/justice/article-29/documentation/opinion-recommendation/files/2014/wp216_en.pdf target=_blank rel=noopener>WP 29 Guideline</a></li><li>If full anonymization is not possible, reduce the granularity of the data in your dataset if you aim to produce aggregate insights (e.g. reduce lat/long to 2 decimal points if city-level precision is enough for your purpose or remove the last octets of an ip address, round timestamps to the hour)</li><li>Use less data where possible (e.g. if 10k records are sufficient for an experiment, do not use 1 million)</li><li>Delete data as soon as possible when it is no longer useful (e.g. data from 7 years ago may not be relevant for your model)</li><li>Remove links in your dataset (e.g. obfuscate user IDs, device identifiers, and other linkable attributes)</li><li>Minimize the number of stakeholders who access the data on a &ldquo;need to know&rdquo; basis</li></ul><p>There are also privacy-preserving techniques being developed that support data minimization:</p><ul><li>distributed data analysis: exchange anonymous aggregated data</li><li>secure multi-party computation: store data distributed-encrypted</li></ul><p>Further reading:</p><ul><li><a href=https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/artificial-intelligence/guidance-on-ai-and-data-protection/ target=_blank rel=noopener>ICO guidance on AI and data protection</a></li><li><a href=https://fpf.org/blog/fpf-report-automated-decision-making-under-the-gdpr-a-comprehensive-case-law-analysis/ target=_blank rel=noopener>FPF case-law analysis on automated decision making</a></li></ul><h2>4. Transparency<span class="absolute -mt-20" id=4-transparency></span>
<a href=#4-transparency class=subheading-anchor aria-label="Permalink for this section"></a></h2><p>Privacy standards such as FIPP or ISO29100 refer to maintaining privacy notices, providing a copy of users data upon request, giving notice when major changes in personal data processing occur, etc.</p><p>GDPR also refers to such practices but also has a specific clause related to algorithmic-decision making.
GDPR&rsquo;s <a href=https://ec.europa.eu/newsroom/article29/items/612053 target=_blank rel=noopener>Article 22</a> allows individuals specific rights under specific conditions. This includes getting a human intervention to an algorithmic decision, an ability to contest the decision, and get a meaningful information about the logic involved. For examples of &ldquo;meaningful information&rdquo;, see EDPS&rsquo;s <a href=https://ec.europa.eu/newsroom/article29/items/612053 target=_blank rel=noopener>guideline</a>. The US <a href=https://www.consumerfinance.gov/about-us/newsroom/cfpb-acts-to-protect-the-public-from-black-box-credit-models-using-complex-algorithms/ target=_blank rel=noopener>Equal Credit Opportunity Act</a> requires detailed explanations on individual decisions by algorithms that deny credit.</p><p>Transparency is not only needed for the end-user. Your models and datasets should be understandable by internal stakeholders as well: model developers, internal audit, privacy engineers, domain experts, and more. This typically requires the following:</p><ul><li>proper model documentation: model type, intent, proposed features, feature importance, potential harm, and bias</li><li>dataset transparency: source, lawful basis, type of data, whether it was cleaned, age. Data cards is a popular approach in the industry to achieve some of these goals. See Google Research&rsquo;s <a href=https://arxiv.org/abs/2204.01075 target=_blank rel=noopener>paper</a> and Meta&rsquo;s <a href=https://ai.facebook.com/research/publications/system-level-transparency-of-machine-learning target=_blank rel=noopener>research</a>.</li><li>traceability: which model has made that decision about an individual and when?</li><li>explainability: several methods exist to make black-box models more explainable. These include LIME, SHAP, counterfactual explanations, Deep Taylor Decomposition, etc. See also <a href=https://github.com/jphall663/awesome-machine-learning-interpretability target=_blank rel=noopener>this overview of machine learning interpretability</a> and <a href=https://www.softwareimprovementgroup.com/resources/unraveling-the-incomprehensible-the-pros-and-cons-of-explainable-ai/ target=_blank rel=noopener>this article on the pros and cons of explainable AI</a>.</li></ul><h2>5. Privacy Rights<span class="absolute -mt-20" id=5-privacy-rights></span>
<a href=#5-privacy-rights class=subheading-anchor aria-label="Permalink for this section"></a></h2><p>Also known as &ldquo;individual participation&rdquo; under privacy standards, this principle allows individuals to submit requests to your organization related to their personal data. Most referred rights are:</p><ol><li>right to access/portability: provide a copy of user data, preferably in a machine-readable format. If data is properly anonymized, it may be exempted from this right.</li><li>right to erasure: erase user data unless an exception applies. It is also a good practice to re-train your model without the deleted user&rsquo;s data.</li><li>right to correction: allow users to correct factually incorrect data. Also, see accuracy below</li><li>right to object: allow users to object to the usage of their data for a specific use (e.g. model training)</li></ol><h2>6. Data accuracy<span class="absolute -mt-20" id=6-data-accuracy></span>
<a href=#6-data-accuracy class=subheading-anchor aria-label="Permalink for this section"></a></h2><p>You should ensure that your data is correct as the output of an algorithmic decision with incorrect data may lead to severe consequences for the individual. For example, if the user&rsquo;s phone number is incorrectly added to the system and if such number is associated with fraud, the user might be banned from a service/system in an unjust manner. You should have processes/tools in place to fix such accuracy issues as soon as possible when a proper request is made by the individual.</p><p>To satisfy the accuracy principle, you should also have tools and processes in place to ensure that the data is obtained from reliable sources, its validity and correctness claims are validated, and data quality and accuracy are periodically assessed.</p><h2>7. Consent<span class="absolute -mt-20" id=7-consent></span>
<a href=#7-consent class=subheading-anchor aria-label="Permalink for this section"></a></h2><p>Consent may be used or required in specific circumstances. In such cases, consent must satisfy the following:</p><ol><li>obtained before collecting, using, updating, or sharing the data</li><li>consent should be recorded and be auditable</li><li>consent should be granular (use consent per purpose, and avoid blanket consent)</li><li>consent should not be bundled with T&amp;S</li><li>consent records should be protected from tampering</li><li>consent method and text should adhere to specific requirements of the jurisdiction in which consent is required (e.g. GDPR requires unambiguous, freely given, written in clear and plain language, explicit and withdrawable)</li><li>Consent withdrawal should be as easy as giving consent</li><li>If consent is withdrawn, then all associated data with the consent should be deleted and the model should be re-trained.</li></ol><p>Please note that consent will not be possible in specific circumstances (e.g. you cannot collect consent from a fraudster, and an employer cannot collect consent from an employee as there is a power imbalance). If you must collect consent, then ensure that it is properly obtained, recorded and proper actions are taken if it is withdrawn.</p><h2>8. Model attacks<span class="absolute -mt-20" id=8-model-attacks></span>
<a href=#8-model-attacks class=subheading-anchor aria-label="Permalink for this section"></a></h2><p>See the security section for security threats to data confidentiality, as they of course represent a privacy risk if that data is personal data. Notable: membership inference, model inversion, and training data leaking from the engineering process. In addition, models can disclose sensitive data that was unintentionally stored during training.</p><h2>Scope boundaries of AI privacy<span class="absolute -mt-20" id=scope-boundaries-of-ai-privacy></span>
<a href=#scope-boundaries-of-ai-privacy class=subheading-anchor aria-label="Permalink for this section"></a></h2><p>As said, many of the discussion topics on AI are about human rights, social justice, safety and only a part of it has to do with privacy. So as a data protection officer or engineer it&rsquo;s important not to drag everything into your responsibilities. At the same time, organizations do need to assign those non-privacy AI responsibilities somewhere.</p><h2>Before you start: Privacy restrictions on what you can do with AI<span class="absolute -mt-20" id=before-you-start-privacy-restrictions-on-what-you-can-do-with-ai></span>
<a href=#before-you-start-privacy-restrictions-on-what-you-can-do-with-ai class=subheading-anchor aria-label="Permalink for this section"></a></h2><p>The GDPR does not restrict the applications of AI explicitly but does provide safeguards that may limit what you can do, in particular regarding lawfulness and limitations on purposes of collection, processing, and storage - as mentioned above. For more information on lawful grounds, see <a href=https://gdpr.eu/article-6-how-to-process-personal-data-legally/ target=_blank rel=noopener>article 6</a></p><p>The <a href=https://www.ftc.gov/business-guidance/blog/2023/02/keep-your-ai-claims-check target=_blank rel=noopener>US Federal Trade Committee</a> provides some good (global) guidance in communicating carefully about your AI, including not to overpromise.</p><p>The <a href="https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:52021PC0206&amp;from=EN" target=_blank rel=noopener>EU AI act</a> does pose explicit application limitations, such as mass surveillance, predictive policing, and restrictions on high-risk purposes such as selecting people for jobs. In addition, there are regulations for specific domains that restrict the use of data, putting limits to some AI approaches (e.g. the medical domain).</p><p><strong>The EU AI Act in a nutshell:</strong></p><p>Safety, health and fundamental rights are at the core of the AI Act, so risks are analyzed from a perspective of harmfulness to people.</p><p>The Act identifies four risk levels for AI systems:</p><ul><li><strong>Unacceptable risk</strong>: will be banned. Includes: Manipulation of people, social scoring, and real-time remote biometric identification (e.g. face recognition with cameras in public space).</li><li><strong>High risk</strong>: products already under safety legislation, plus eight areas (including critical infrastructure and law enforcement). These systems need to comply with a number of rules including the security risk assessment and conformity with harmonized (adapted) AI security standards OR the essential requirements of the Cyber Resilience Act (when applicable).</li><li><strong>Limited risk</strong>: has limited potential for manipulation. Should comply with minimal transparency requirements to users that would allow users to make informed decisions. After interacting with the applications, the user can then decide whether they want to continue using it.</li><li><strong>Minimal/non risk</strong>: the remaining systems.</li></ul><p>So organizations will have to know their AI initiatives and perform high-level risk analysis to determine the risk level.</p><p>AI is broadly defined here and includes wider statistical approaches and optimization algorithms.</p><p>Generative AI needs to disclose what copyrighted sources were used, and prevent illegal content. To illustrate: if OpenAI for example would violate this rule, they could face a 10 billion dollar fine.</p><p>Links:</p><ul><li><a href=https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence target=_blank rel=noopener>AI Act</a></li><li><a href=https://digital-strategy.ec.europa.eu/en/library/commission-publishes-guidelines-prohibited-artificial-intelligence-ai-practices-defined-ai-act target=_blank rel=noopener>Guidelines on prohibited AI</a></li><li><a href=https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai target=_blank rel=noopener>AI Act page of the EU</a></li></ul><h2>Further reading on AI privacy<span class="absolute -mt-20" id=further-reading-on-ai-privacy></span>
<a href=#further-reading-on-ai-privacy class=subheading-anchor aria-label="Permalink for this section"></a></h2><ul><li><a href=https://doi.org/10.6028/NIST.AI.100-1 target=_blank rel=noopener>NIST AI Risk Management Framework 1.0</a></li><li><a href=https://plot4.ai/library target=_blank rel=noopener>PLOT4ai threat library</a></li><li><a href=https://algorithmaudit.eu/ target=_blank rel=noopener>Algorithm audit non-profit organisation</a></li><li>For pure security aspects: see the &lsquo;Further reading on AI security&rsquo; above in this document</li></ul></div></div></main><aside class=docs-toc><div class=toc-container><h3 class=toc-title>On this page</h3><div class="toc-content bg-gradient-to-b from-[#CCF6CE] to-[#FDFBFB] border border-gray-300 rounded-lg p-4"><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a><ul><li><a href=#privacy-concerns-of-ai-systems>Privacy concerns of AI systems</a></li><li><a href=#privacy--personal-data-protection--respect-for-further-individual-rights>Privacy = personal data protection + respect for further individual rights</a></li><li><a href=#legislation>Legislation</a></li><li><a href=#assessments>Assessments</a></li></ul></li><li><a href=#1-use-limitation-and-purpose-specification>1. Use Limitation and Purpose Specification</a></li><li><a href=#2-fairness>2. Fairness</a></li><li><a href=#3-data-minimization-and-storage-limitation>3. Data Minimization and Storage Limitation</a></li><li><a href=#4-transparency>4. Transparency</a></li><li><a href=#5-privacy-rights>5. Privacy Rights</a></li><li><a href=#6-data-accuracy>6. Data accuracy</a></li><li><a href=#7-consent>7. Consent</a></li><li><a href=#8-model-attacks>8. Model attacks</a></li><li><a href=#scope-boundaries-of-ai-privacy>Scope boundaries of AI privacy</a></li><li><a href=#before-you-start-privacy-restrictions-on-what-you-can-do-with-ai>Before you start: Privacy restrictions on what you can do with AI</a></li><li><a href=#further-reading-on-ai-privacy>Further reading on AI privacy</a></li></ul></nav></div></div></aside></div><section class="relative bg-[#EDF7ED] py-16"><div class="max-w-[1400px] mx-auto px-6 md:px-12"><h2 class="text-3xl md:text-4xl font-bold text-center text-[#1a1a2e] mb-16">We are always happy to assist you!</h2><div class="grid grid-cols-1 md:grid-cols-2 gap-12 items-start"><div><div class="flex flex-wrap gap-4 mb-8"><a href=# class="w-10 h-10 bg-white rounded-md flex items-center justify-center shadow"><i class="fab fa-github text-gray-700"></i></a>
<a href=# class="w-10 h-10 bg-white rounded-md flex items-center justify-center shadow"><i class="fab fa-youtube text-red-600"></i></a>
<a href=# class="w-10 h-10 bg-white rounded-md flex items-center justify-center shadow"><i class="fab fa-linkedin-in text-blue-700"></i></a>
<a href=# class="w-10 h-10 bg-white rounded-md flex items-center justify-center shadow"><img src=/images/slack.png alt=Slack class="w-6 h-6"></a></div><h3 class="text-2xl font-bold text-[#1a1a2e] mb-4">Send us a message</h3><p class="text-gray-500 leading-relaxed">Have questions about AI security? Want to contribute to our mission?
We'd love to hear from you. Reach out through any of our channels or
use the contact form.</p></div><div><form class="flex flex-col gap-6"><div class="grid grid-cols-1 md:grid-cols-2 gap-4"><input type=text placeholder="First Name*" class="p-4 border border-gray-300 rounded-lg text-base w-full">
<input type=text placeholder="Last Name*" class="p-4 border border-gray-300 rounded-lg text-base w-full"></div><input type=email placeholder="Email Address*" class="p-4 border border-gray-300 rounded-lg text-base w-full">
<textarea placeholder=Message rows=4 class="p-4 border border-gray-300 rounded-lg text-base resize-vertical w-full"></textarea>
<button type=submit class="bg-green-500 text-white px-6 py-3 rounded-lg font-bold flex items-center justify-center gap-2 w-max hover:bg-green-600 transition">
Submit <span>→</span></button></form></div></div></div></section></main><script src=https://cdn.tailwindcss.com></script>
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css><link href="https://fonts.googleapis.com/css2?family=Inter:wght@500&family=Manrope:wght@600&family=Roboto:wght@500&display=swap" rel=stylesheet><script>tailwind.config={theme:{extend:{fontFamily:{inter:["Inter","sans-serif"],manrope:["Manrope","sans-serif"],roboto:["Roboto","sans-serif"]}}}}</script><footer class="bg-black text-white w-full py-6"><div class="container flex flex-col p-2 px-10"><div class="flex justify-between items-start"><div class="flex flex-col logo-section"><a href=/ class=logo-link><img src=/images/Owasp-AI-Exchange-Logo.png alt="OWASP AI Exchange" class="site-logo h-[39px] w-auto"></a><ul class="flex space-x-8 mt-4 font-roboto text-[16px]"><li><a href=/ class=hover:text-white>Home</a></li><li><a href=/docs/ai_security_overview/ class=hover:text-white>Overview</a></li><li><a href=/media/ class=hover:text-white>Media</a></li><li><a href=/sponsor/ class=hover:text-white>Sponsor</a></li><li><a href=/contribute/ class=hover:text-white>Contribute</a></li><li><a href=/connect/ class=hover:text-white>Connect</a></li></ul></div><div class="flex space-x-4 mt-10"><a href=# class="w-9 h-9 flex items-center justify-center border border-white rounded-full hover:bg-gray-800 transition"><i class="fab fa-github"></i></a>
<a href=# class="w-9 h-9 flex items-center justify-center border border-white rounded-full hover:bg-gray-800 transition"><i class="fab fa-slack"></i></a>
<a href=# class="w-9 h-9 flex items-center justify-center border border-white rounded-full hover:bg-gray-800 transition"><i class="fab fa-linkedin-in"></i></a>
<a href=# class="w-9 h-9 flex items-center justify-center border border-white rounded-full hover:bg-gray-800 transition"><i class="fab fa-youtube"></i></a></div></div><div class="border-t border-gray-600 mt-4"></div><div class="flex w-full flex-row justify-between space-between pt-6"><p class="font-roboto text-[16px]">Copyright © owasp.org 2025. All Rights Reserved.</p><p class="flex gap-2 text-[12px]">PROUDLY DESIGNED AND DEVELOPED BY:
<img src=/images/usability.png alt="Usability Icon" class="h-5 inline-block">
<a href=https://usabilitydesigns.com/ class=text-[#FFAE3C]>USABILITY DESIGNS</a></p></div></div></footer></body></html>