<!doctype html><html lang=en><head><meta property="og:title" content="1. General controls – AI Exchange"><meta property="og:description" content="Comprehensive guidance and alignment on how to protect AI against security threats - by professionals, for professionals."><meta property="og:type" content="article"><meta property="og:url" content="https://owaspai.org/docs/1_general_controls/"><meta property="og:image" content="https://owaspai.org/images/aix-og-logo.jpg"><meta property="article:section" content="docs"><meta charset=utf-8><script src=https://cdn.tailwindcss.com></script><meta name=viewport content="width=device-width,initial-scale=1"><title>1. General controls | AI Exchange</title><meta name=description content><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Roboto:ital,wght@0,100..900;1,100..900&display=swap" rel=stylesheet><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.2/css/all.min.css><link rel=stylesheet href="/css/site.css?v=2"><link rel=stylesheet href="/css/custom-style.css?v=2"><script src=/js/script.js></script>
<link href=https://unpkg.com/aos@2.3.1/dist/aos.css rel=stylesheet><script src=https://unpkg.com/aos@2.3.1/dist/aos.js></script>
<script>AOS.init({delay:0,disable:"phone"})</script></head><body><header class=main-header><div class=header-background></div><div class="container header-container"><div class=logo-section><a href=/ class=logo-link><img src=/images/Owasp-AI-Exchange-Logo.png alt="AI Exchange" class=site-logo width=Auto height=48></a></div><nav class="main-navigation desktop-nav"><ul class=nav-list><li class=nav-item><a href=/ class=nav-link>Home</a></li><li class=nav-item><a href=/docs/ai_security_overview/ class=nav-link>Content</a></li><li class=nav-item><a href=/media/ class=nav-link>Media</a></li><li class=nav-item><a href=/contribute/ class=nav-link>Contribute</a></li><li class=nav-item><a href=/connect/ class=nav-link>Connect</a></li><li class=nav-item><a href=/sponsor/ class=nav-link>Sponsor</a></li></ul></nav><div class=header-actions><div class="social-icons desktop-social"><a href=https://github.com/OWASP/www-project-ai-security-and-privacy-guide class=social-link title=GitHub><svg class="social-icon" viewBox="0 0 24 24" fill="currentcolor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23A11.509 11.509.0 0112 5.803c1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg></a><a href=https://owasp.org/slack/invite class=social-link title=Slack><svg class="social-icon" viewBox="0 0 24 24" fill="currentcolor"><path d="M5.042 15.165a2.528 2.528.0 01-2.52 2.523A2.528 2.528.0 010 15.165a2.527 2.527.0 012.522-2.52h2.52v2.52zm1.271.0a2.527 2.527.0 012.521-2.52 2.527 2.527.0 012.521 2.52v6.313A2.528 2.528.0 018.834 24a2.528 2.528.0 01-2.521-2.522v-6.313zM8.834 5.042a2.528 2.528.0 01-2.521-2.52A2.528 2.528.0 018.834.0a2.528 2.528.0 012.521 2.522v2.52H8.834zm0 1.271a2.528 2.528.0 012.521 2.521 2.528 2.528.0 01-2.521 2.521H2.522A2.528 2.528.0 010 8.834a2.528 2.528.0 012.522-2.521h6.312zM18.956 8.834a2.528 2.528.0 012.522-2.521A2.528 2.528.0 0124 8.834a2.528 2.528.0 01-2.522 2.521h-2.522V8.834zm-1.268.0a2.528 2.528.0 01-2.523 2.521 2.527 2.527.0 01-2.52-2.521V2.522A2.527 2.527.0 0115.165.0a2.528 2.528.0 012.523 2.522v6.312zM15.165 18.956a2.528 2.528.0 012.523 2.522A2.528 2.528.0 0115.165 24a2.527 2.527.0 01-2.52-2.522v-2.522h2.52zm0-1.268a2.527 2.527.0 01-2.52-2.523 2.526 2.526.0 012.52-2.52h6.313A2.527 2.527.0 0124 15.165a2.528 2.528.0 01-2.522 2.523h-6.313z"/></svg></a><a href=https://www.linkedin.com/company/owasp-ai-exchange/ class=social-link title=LinkedIn><svg class="social-icon" viewBox="0 0 24 24" fill="currentcolor"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853.0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601.0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144.0-2.063-.926-2.063-2.065.0-1.138.92-2.063 2.063-2.063 1.14.0 2.064.925 2.064 2.063.0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225.0H1.771C.792.0.0.774.0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2.0 22.222.0h.003z"/></svg></a><a href="https://www.youtube.com/playlist?list=PLCZNSZ1gyRoD5droM_qyXYyBofj00x_zO" class=social-link title=Youtube><svg class="social-icon" viewBox="0 0 24 24" fill="currentcolor"><path d="M23.498 6.186A3.016 3.016.0 0021.376 4.05C19.505 3.545 12 3.545 12 3.545s-7.505.0-9.377.505A3.017 3.017.0 00.502 6.186C0 8.07.0 12 0 12s0 3.93.502 5.814a3.016 3.016.0 002.122 2.136c1.871.505 9.376.505 9.376.505s7.505.0 9.377-.505a3.015 3.015.0 002.122-2.136C24 15.93 24 12 24 12s0-3.93-.502-5.814zM9.545 15.568V8.432L15.818 12l-6.273 3.568z"/></svg></a></div><button class=search-icon id=search-toggle aria-label=Search><svg class="search-svg" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2"><circle cx="11" cy="11" r="8"/><path d="m21 21-4.35-4.35"/></svg></button>
<button class=hamburger aria-label="Toggle menu">
<span></span>
<span></span>
<span></span></button></div></div></header><div class=search-overlay id=searchOverlay><div class=search-container><div class=search-header><button class=search-close id=search-close aria-label="Close search"><svg viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2"><line x1="18" y1="6" x2="6" y2="18"/><line x1="6" y1="6" x2="18" y2="18"/></svg></button></div><div id=pagefind-search class=pagefind-search-container></div></div></div><div class=mobile-menu-overlay id=mobileMenu><div class=mobile-menu-header><div class=mobile-logo-section><a href=/ class=mobile-logo-link><img src=/images/Owasp-AI-Exchange-Logo.png alt="AI Exchange" class=mobile-site-logo width=Auto height=48></a></div><button class=mobile-menu-close aria-label="Close menu"><svg class="close-icon" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2"><line x1="18" y1="6" x2="6" y2="18"/><line x1="6" y1="6" x2="18" y2="18"/></svg></button></div><nav class=mobile-navigation><ul class=mobile-nav-list><li class=mobile-nav-item><a href=/ class=mobile-nav-link>Home</a></li><li class=mobile-nav-item><a href=/docs/ai_security_overview/ class=mobile-nav-link>Content</a></li><li class=mobile-nav-item><a href=/media/ class=mobile-nav-link>Media</a></li><li class=mobile-nav-item><a href=/contribute/ class=mobile-nav-link>Contribute</a></li><li class=mobile-nav-item><a href=/connect/ class=mobile-nav-link>Connect</a></li><li class=mobile-nav-item><a href=/sponsor/ class=mobile-nav-link>Sponsor</a></li><li class=mobile-nav-item><button class="mobile-nav-link text-center mobile-search-btn" onclick='document.getElementById("search-toggle").click()'><svg class="search-svg inline" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2"><circle cx="11" cy="11" r="8"/><path d="m21 21-4.35-4.35"/></svg> Search</button></li></ul></nav><div class=mobile-social-icons><a href=https://github.com/OWASP/www-project-ai-security-and-privacy-guide class=mobile-social-link title=GitHub><svg class="mobile-social-icon" viewBox="0 0 24 24" fill="currentcolor"><path d="M12 0C5.374.0.0 5.373.0 12c0 5.302 3.438 9.8 8.207 11.387.599.111.793-.261.793-.577v-2.234c-3.338.726-4.033-1.416-4.033-1.416-.546-1.387-1.333-1.756-1.333-1.756-1.089-.745.083-.729.083-.729 1.205.084 1.839 1.237 1.839 1.237 1.07 1.834 2.807 1.304 3.492.997.107-.775.418-1.305.762-1.604-2.665-.305-5.467-1.334-5.467-5.931.0-1.311.469-2.381 1.236-3.221-.124-.303-.535-1.524.117-3.176.0.0 1.008-.322 3.301 1.23A11.509 11.509.0 0112 5.803c1.02.005 2.047.138 3.006.404 2.291-1.552 3.297-1.23 3.297-1.23.653 1.653.242 2.874.118 3.176.77.84 1.235 1.911 1.235 3.221.0 4.609-2.807 5.624-5.479 5.921.43.372.823 1.102.823 2.222v3.293c0 .319.192.694.801.576C20.566 21.797 24 17.3 24 12c0-6.627-5.373-12-12-12z"/></svg></a><a href=https://www.linkedin.com/company/owasp-ai-exchange/ class=mobile-social-link title=LinkedIn><svg class="mobile-social-icon" viewBox="0 0 24 24" fill="currentcolor"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853.0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601.0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144.0-2.063-.926-2.063-2.065.0-1.138.92-2.063 2.063-2.063 1.14.0 2.064.925 2.064 2.063.0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225.0H1.771C.792.0.0.774.0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2.0 22.222.0h.003z"/></svg></a><a href="https://www.youtube.com/playlist?list=PLCZNSZ1gyRoD5droM_qyXYyBofj00x_zO" class=mobile-social-link title=Youtube><svg class="mobile-social-icon" viewBox="0 0 24 24" fill="currentcolor"><path d="M23.498 6.186A3.016 3.016.0 0021.376 4.05C19.505 3.545 12 3.545 12 3.545s-7.505.0-9.377.505A3.017 3.017.0 00.502 6.186C0 8.07.0 12 0 12s0 3.93.502 5.814a3.016 3.016.0 002.122 2.136c1.871.505 9.376.505 9.376.505s7.505.0 9.377-.505a3.015 3.015.0 002.122-2.136C24 15.93 24 12 24 12s0-3.93-.502-5.814zM9.545 15.568V8.432L15.818 12l-6.273 3.568z"/></svg></a></div></div><style>.main-navigation.desktop-nav{display:flex}.search-icon{background:0 0;border:none;cursor:pointer;padding:0;display:flex;align-items:center;justify-content:center}.search-icon:hover{opacity:.8}.search-svg{width:20px;height:20px;stroke:currentColor}.hamburger{display:none;flex-direction:column;justify-content:space-between;width:24px;height:18px;background:0 0;border:none;cursor:pointer;z-index:1001}.hamburger span{display:block;height:2px;width:100%;background:#fff;border-radius:2px;transition:all .3s ease}.hamburger.active span:nth-child(1){transform:rotate(45deg)translate(5px,5px)}.hamburger.active span:nth-child(2){opacity:0}.hamburger.active span:nth-child(3){transform:rotate(-45deg)translate(7px,-6px)}.mobile-menu-overlay{position:fixed;top:0;left:0;width:100vw;height:100vh;background:#002843;background-image:radial-gradient(circle at 25% 25%,rgba(255,255,255,.1) 1px,transparent 1px),radial-gradient(circle at 75% 75%,rgba(255,255,255,.1) 1px,transparent 1px),radial-gradient(circle at 50% 50%,rgba(255,255,255,.1) 1px,transparent 1px),radial-gradient(circle at 10% 90%,rgba(255,255,255,.1) 1px,transparent 1px),radial-gradient(circle at 90% 10%,rgba(255,255,255,.1) 1px,transparent 1px);background-size:50px 50px,80px 80px,60px 60px,40px 40px,70px 70px;z-index:9999;display:none;flex-direction:column;justify-content:space-between;padding:0;overflow:hidden}.mobile-menu-overlay.active{display:flex}.mobile-menu-header{display:flex;justify-content:space-between;align-items:center;padding:2rem 2rem 1rem;border-bottom:1px solid rgba(255,255,255,.1);position:relative}.mobile-menu-header::before{content:'';position:absolute;top:0;left:0;right:0;height:1px;background:rgba(255,255,255,.2)}.mobile-logo-section{flex:1}.mobile-site-logo{height:48px;width:auto;filter:brightness(0)invert(1)}.mobile-site-title{color:#fff;font-size:1.5rem;font-weight:700;text-transform:uppercase;letter-spacing:.05em}.mobile-site-title-link{text-decoration:none}.mobile-menu-close{background:0 0;border:none;cursor:pointer;padding:.5rem;border-radius:50%;transition:all .3s ease}.mobile-menu-close:hover{background:rgba(255,255,255,.1)}.close-icon{width:24px;height:24px;stroke:#fff;stroke-width:2}.mobile-navigation{flex:1;display:flex;align-items:center;justify-content:center;padding:2rem}.mobile-nav-list{list-style:none;margin:0;padding:0;width:100%;max-width:400px}.mobile-nav-item{margin:0}.mobile-nav-link{display:block;color:#fff;text-decoration:none;font-weight:500;font-size:1.25rem;padding:1.5rem 1rem;margin:.5rem 1rem;border-radius:8px;transition:all .3s ease;position:relative;text-align:center}.mobile-nav-link:hover{color:rgba(255,255,255,.8);background:rgba(255,255,255,.1)}.mobile-nav-link.active{background:#4caf50;color:#fff;font-weight:700}.mobile-nav-link.active:hover{background:#45a049;color:#fff}.mobile-social-icons{display:flex;justify-content:center;gap:2rem;padding:2rem;border-top:1px solid rgba(255,255,255,.1)}.mobile-social-link{color:#fff;text-decoration:none;transition:all .3s ease;display:flex;align-items:center;justify-content:center;width:48px;height:48px;border-radius:50%;background:rgba(255,255,255,.1)}.mobile-social-link:hover{color:#93c5fd;background:rgba(255,255,255,.2);transform:translateY(-2px)}.mobile-social-icon{width:24px;height:24px;fill:currentColor}@media(max-width:768px){.desktop-nav{display:none!important}.desktop-social{display:none!important}.hamburger{display:flex}.search-icon{display:flex}.header-container{flex-direction:row;justify-content:space-between;align-items:center;padding-left:1rem!important;padding-right:1.5rem!important}.logo-section{flex:1}.header-actions{display:flex;align-items:center;gap:1rem}.site-logo{height:40px;width:auto;filter:brightness(0)invert(1)}.site-title{font-size:1.25rem}}@media(min-width:769px){.mobile-menu-overlay{display:none!important}}.search-overlay{position:fixed;top:0;left:0;width:100vw;height:100vh;background:rgba(0,0,0,.95);backdrop-filter:blur(10px);z-index:9998;display:none;padding:2rem}.search-overlay.active{display:flex;flex-direction:column}.search-container{max-width:800px;width:100%;margin:0 auto}.search-header{display:flex;justify-content:flex-end;gap:1rem;margin-bottom:1rem}.search-close{width:48px;height:48px;display:flex;align-items:center;justify-content:center;background:rgba(255,255,255,.1);border:1px solid rgba(255,255,255,.2);border-radius:8px;color:#fff;cursor:pointer;transition:all .3s ease}.search-close:hover{background:rgba(255,255,255,.2)}.search-close svg{width:24px;height:24px}.pagefind-search-container{--pagefind-ui-scale:1;--pagefind-ui-primary:#4CAF50;--pagefind-ui-text:#fff;--pagefind-ui-background:rgba(255, 255, 255, 0.05);--pagefind-ui-border:rgba(255, 255, 255, 0.2);--pagefind-ui-border-width:1px;--pagefind-ui-border-radius:8px;--pagefind-ui-font:inherit;max-height:calc(100vh - 180px);overflow-y:auto}.mobile-search-btn{background:0 0;border:none;cursor:pointer;width:100%;margin:0}</style><link href=/pagefind/pagefind-ui.css rel=stylesheet><script src=/pagefind/pagefind-ui.js></script>
<script>document.addEventListener("DOMContentLoaded",()=>{const t=document.querySelector(".hamburger"),e=document.querySelector(".mobile-menu-overlay"),n=document.querySelector(".mobile-menu-close"),s=document.querySelectorAll(".mobile-nav-link");t.addEventListener("click",()=>{e.classList.add("active"),t.classList.add("active"),document.body.style.overflow="hidden"}),n.addEventListener("click",()=>{e.classList.remove("active"),t.classList.remove("active"),document.body.style.overflow="auto"}),s.forEach(n=>{n.addEventListener("click",()=>{e.classList.remove("active"),t.classList.remove("active"),document.body.style.overflow="auto"})}),e.addEventListener("click",n=>{n.target===e&&(e.classList.remove("active"),t.classList.remove("active"),document.body.style.overflow="auto")}),document.addEventListener("keydown",n=>{n.key==="Escape"&&e.classList.contains("active")&&(e.classList.remove("active"),t.classList.remove("active"),document.body.style.overflow="auto")})}),document.addEventListener("DOMContentLoaded",()=>{const t=document.getElementById("search-toggle"),e=document.getElementById("searchOverlay"),n=document.getElementById("search-close");typeof PagefindUI!="undefined"&&new PagefindUI({element:"#pagefind-search",translations:{placeholder:"Search articles, guides, and content..."},showSubResults:!0,showImages:!1,resetStyles:!1,autofocus:!0}),t&&t.addEventListener("click",t=>{t.preventDefault(),e&&(e.classList.add("active"),document.body.style.overflow="hidden")}),n&&n.addEventListener("click",()=>{e.classList.remove("active"),document.body.style.overflow="auto"}),document.addEventListener("keydown",t=>{t.key==="Escape"&&e&&e.classList.contains("active")&&(e.classList.remove("active"),document.body.style.overflow="auto")})})</script><main id=main><script src=https://unpkg.com/alpinejs@3.x.x/dist/cdn.min.js defer></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.2/css/all.min.css><section class="intro-banner text-white py-12 mb-0 bg-cover bg-center bg-no-repeat" style=background-image:url(/images/overview-hero.png)><div class="container text-left bg-opacity-50 p-8 rounded-lg"><h1 class="text-[46px] md:text-5xl font-bold mb-4">General controls</h1><p class="text-lg text-gray-300 max-w-4xl">These are the controls every organisation should have in place...</p></div></section><div class=docs-layout x-data="{ openSidebar: false, openToc: false }"><div class="md:hidden flex flex-col gap-3 p-4"><div class="bg-[#EEF3FF] rounded-xl shadow-sm"><button @click="openSidebar = !openSidebar" class="w-full flex justify-between items-center text-gray-800 font-semibold px-5 py-3">
<span>Other pages</span><svg xmlns="http://www.w3.org/2000/svg" :class="{'rotate-90': openSidebar}" class="w-5 h-5 text-gray-600 transform transition-transform" fill="none" viewBox="0 0 24 24" stroke="currentcolor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7"/></svg></button><div x-show=openSidebar x-transition class="px-5 pb-3"><ul class="flex flex-col text-left text-sm font-medium text-gray-700 space-y-2"><li><a href=/docs/ai_security_overview/ class=hover:text-[#006284]>0. AI Security Overview</a></li><li><a href=/docs/1_general_controls/ class=hover:text-[#006284]>1. General Controls</a></li><li><a href=/docs/2_threats_through_use/ class=hover:text-[#006284]>2. Input Threats</a></li><li><a href=/docs/3_development_time_threats/ class=hover:text-[#006284]>3. Development-Time Threats</a></li><li><a href=/docs/4_runtime_application_security_threats/ class=hover:text-[#006284]>4. Runtime Conventional Security Threats</a></li><li><a href=/docs/5_testing/ class=hover:text-[#006284]>5. AI Security Testing</a></li><li><a href=/docs/6_privacy/ class=hover:text-[#006284]>6. AI Privacy</a></li><li><a href=/docs/ai_security_references/ class=hover:text-[#006284]>AI Security References</a></li><li><a href=/docs/ai_security_index class=hover:text-[#006284]>Index</a></li></ul></div></div><div class="bg-[#EEF3FF] rounded-xl shadow-sm"><button @click="openToc = !openToc" class="w-full flex justify-between items-center text-gray-800 font-semibold px-5 py-3">
<span>Topics on this page</span><svg xmlns="http://www.w3.org/2000/svg" :class="{'rotate-90': openToc}" class="w-5 h-5 text-gray-600 transform transition-transform" fill="none" viewBox="0 0 24 24" stroke="currentcolor"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M9 5l7 7-7 7"/></svg></button><div x-show=openToc x-transition class="px-5 pb-3"><div class="text-left text-sm text-gray-700 space-y-2"><nav id=TableOfContents><ul><li><a href=#11-general-governance-controls>1.1 General governance controls</a><ul><li><ul><li><a href=#ai-program>#AI PROGRAM</a></li><li><a href=#sec-program>#SEC PROGRAM</a></li><li><a href=#sec-dev-program>#SEC DEV PROGRAM</a></li><li><a href=#dev-program>#DEV PROGRAM</a></li><li><a href=#check-compliance>#CHECK COMPLIANCE</a></li><li><a href=#sec-educate>#SEC EDUCATE</a></li></ul></li></ul></li><li><a href=#12-general-controls-for-sensitive-data-limitation>1.2 General controls for sensitive data limitation</a><ul><li><ul><li><a href=#data-minimize>#DATA MINIMIZE</a></li><li><a href=#allowed-data>#ALLOWED DATA</a></li><li><a href=#short-retain>#SHORT RETAIN</a></li><li><a href=#obfuscate-training-data>#OBFUSCATE TRAINING DATA</a></li><li><a href=#discrete>#DISCRETE</a></li></ul></li></ul></li><li><a href=#13-controls-to-limit-the-effects-of-unwanted-behaviour>1.3. Controls to limit the effects of unwanted behaviour</a><ul><li><ul><li><a href=#oversight>#OVERSIGHT</a></li><li><a href=#least-model-privilege>#LEAST MODEL PRIVILEGE</a></li><li><a href=#model-alignment>#MODEL ALIGNMENT</a></li><li><a href=#ai-transparency>#AI TRANSPARENCY</a></li><li><a href=#continuous-validation>#CONTINUOUS VALIDATION</a></li><li><a href=#explainability>#EXPLAINABILITY</a></li><li><a href=#unwanted-bias-testing>#UNWANTED BIAS TESTING</a></li></ul></li></ul></li></ul></nav></div></div></div></div><div class="hidden md:flex docs-sidebar-column"><aside class="docs-sidebar p-0"><div class="sidebar-container pt-6"><nav class=sidebar-nav><ul class="flex flex-col"><li><a href=/docs/ai_security_overview/ class=sidebar-nav-link><span>0. AI Security Overview</span></a></li><li><a href=/docs/1_general_controls/ class="sidebar-nav-link active"><span>1. General controls</span></a></li><li><a href=/docs/2_threats_through_use/ class=sidebar-nav-link><span>2. Input threats</span></a></li><li><a href=/docs/3_development_time_threats/ class=sidebar-nav-link><span>3. Development-time threats</span></a></li><li><a href=/docs/4_runtime_application_security_threats/ class=sidebar-nav-link><span>4. Runtime conventional security threats</span></a></li><li><a href=/docs/5_testing/ class=sidebar-nav-link><span>5. AI security testing</span></a></li><li><a href=/docs/6_privacy/ class=sidebar-nav-link><span>6. AI privacy</span></a></li><li><a href=/docs/ai_security_references/ class=sidebar-nav-link><span>AI Security References</span></a></li><li><a href=/docs/ai_security_index class=sidebar-nav-link><span>Index</span></a></li></ul></nav></div></aside></div><main class=docs-main><div class=docs-content><nav class=breadcrumbs><a href=/>Home</a>
<span class=breadcrumb-separator>></span>
<span class=current-page>1. General controls</span></nav><h1 class=docs-title>1. General controls</h1><div class="docs-body documentation"><blockquote><p>Category: group of controls<br>Permalink: <a href=https://owaspai.org/goto/generalcontrols/ target=_blank rel=noopener>https://owaspai.org/goto/generalcontrols/</a></p></blockquote><h2>1.1 General governance controls<span class="absolute -mt-20" id=11-general-governance-controls></span>
<a href=#11-general-governance-controls class=subheading-anchor aria-label="Permalink for this section"></a></h2><blockquote><p>Category: group of controls<br>Permalink: <a href=https://owaspai.org/goto/governancecontrols/ target=_blank rel=noopener>https://owaspai.org/goto/governancecontrols/</a></p></blockquote><h4>#AI PROGRAM<span class="absolute -mt-20" id=ai-program></span>
<a href=#ai-program class=subheading-anchor aria-label="Permalink for this section"></a></h4><blockquote><p>Category: governance control<br>Permalink: <a href=https://owaspai.org/goto/aiprogram/ target=_blank rel=noopener>https://owaspai.org/goto/aiprogram/</a></p></blockquote><p><strong>Description</strong><br>AI program: Install and execute a program to govern AI.<br>One could argue that this control is out of scope for cyber security, but it initiates action to get in control of AI security.</p><p><strong>Objective</strong><br>The objective of an AI Program is to take responsibility for AI as an organization and make sure that all AI initiatives are known and under control, including their security.</p><p><strong>Implementation</strong><br>This governance challenge may seem daunting because of all the new things to take care of, but there are numerous existing controls in organizations already that can be extended to include AI (e.g. policies, risk analysis, impact analysis, inventory of used services etc.).<br>See <a href=/goto/organize/>How to organize AI security</a> for the 5 GUARD steps and to see how governance fits into the whole.<br>An AI Program includes:</p><ul><li>Keeping an inventory of AI initiatives</li><li>Perform impact analysis on initiatives</li><li>Organize AI innovation</li><li>Include AI risks in risk management</li><li>Assign responsibilities, e.g. model accountability, data accountability, and risk governance</li><li>AI literacy (e.g. <a href=/goto/seceducate/>training</a></li><li>Organize <a href=/goto/checkcompliance/>compliance</a></li><li>Incorporate AI assets in the <a href=/goto/secprogram/>security program</a></li></ul><p>When doing impact analysis on AI initiatives, consider at least the following:</p><ul><li>Note that an AI program is not just about risks TO AI, such as security risks - it is also about risks BY AI, such as threats to fairness, safety, etc.</li><li>Include laws and regulations, as the type of AI application may be prohibited (e.g. social scoring under the EU AI Act). See #<a href=/goto/checkcompliance/>CHECKCOMPLIANCE</a></li><li>Can the required transparency be provided into how the AI works?</li><li>Can the privacy rights be achieved (right to access, erase, correct, update personal data, and the right to object)?</li><li>Can unwanted bias regarding protected groups of people be sufficiently mitigated?</li><li>Is AI really needed to solve the problem?</li><li>Is the right expertise available (e.g. data scientists)?</li><li>Is it allowed to use the data for the purpose - especially if it is personal data collected for a different purpose?</li><li>Can unwanted behaviour be sufficiently contained by mitigations (see Controls to limit unwanted behaviour)?</li><li>See Risk management under <a href=/goto/secprogram/>SECPROGRAM</a> for security-specific risk analysis, also involving privacy.</li></ul><p><strong>Quickstart</strong><br>A typical first iteration for AI governance in organizations consists of the following:</p><ol><li>Raise attention and awareness at board level, when needed</li><li>Form a group of stakeholders and assign responsibilities</li><li>Identify laws and regulations</li><li>Send out a survey to make an inventory of current AI use, AI ideas, any concerns, and indiduals with AI expertise</li><li>Evaluate these AI applications and ideas</li><li>Perform a risk analysis and establish a first policy</li><li>Implement policy as much as possible in tools and procedures</li><li>Initiate an AI literacy program, based on the policy implementation plan</li></ol><p><strong>Bare minimum start</strong>
The very minimum first thing you can do for AI governance, focused on security:</p><ol><li>Make an inventory of current AI use and AI ideas.</li><li>Perform <a href=/goto/riskanalysis/>risk analysis</a> on them to identify threats, controls and who&rsquo;s responsible for them.</li><li>Continue with step 2 of the GUARD program, presented in the <a href=/goto/organize/>How to organize</a> section.</li></ol><p><strong>Particularity</strong><br>In general risk management it may help to keep in mind the following particularities of AI:</p><ol><li>Inductive instead of deductive, meaning that being wrong is part of the game for machine learning models, which can lead to harm</li><li>Connected to 1: models can go stale</li><li>Organizes its behaviour based on data, so data becomes a source of opportunity (e.g. complex real-world problem solving, adaptability) and of risk (e.g. unwanted bias, incompleteness, error, manipulation)</li><li>Unfamiliar to organizations and to people, with the risk of implementation mistakes, underreliance, overreliance, and incorrect attribution of human tendencies</li><li>Incomprehensible, resulting in trust issues</li><li>New technical assets that form security threats (data/model supply chain, train data, model parameters, augmentation data, AI documentation)</li><li>Can listen and speak: communicate through natural language instead of user interfaces</li><li>Can hear and see: have sound and vision recognition abilities</li></ol><p><strong>References</strong></p><ul><li><a href=https://www.aigl.blog/ target=_blank rel=noopener>AI Governance library</a></li><li><a href=https://www.unesco.org/ethics-ai/en target=_blank rel=noopener>UNESCO on AI ethics and governance</a></li><li><a href=https://genai.owasp.org/resource/llm-applications-cybersecurity-and-governance-checklist-english/ target=_blank rel=noopener>GenAI security project LLM AI Cybersecurity & governance checklist</a></li></ul><p>Useful standards include:</p><ul><li>ISO/IEC 42001 AI management system. Gap: covers this control fully.</li><li><a href=https://www.federalreserve.gov/supervisionreg/srletters/sr1107.htm target=_blank rel=noopener>US Federal Reserve SR 11-07: Guidance on Model Risk Management</a>: supervisory guidance for banking organizations and supervisors.</li></ul><p>42001 is about extending your risk management system - it focuses on governance. ISO 5338 (see #<a href=/goto/devprogram/>DEV PROGRAM</a> below) is about extending your software lifecycle practices - it focuses on engineering and everything around it. ISO 42001 can be seen as a management system for the governance of responsible AI in an organization, similar to how ISO 27001 is a management system for information security. ISO 42001 doesn&rsquo;t go into the lifecycle processes. For example, it does not discuss how to train models, how to do data lineage, continuous validation, versioning of AI models, project planning challenges, and how and when exactly sensitive data is used in engineering.</p><h4>#SEC PROGRAM<span class="absolute -mt-20" id=sec-program></span>
<a href=#sec-program class=subheading-anchor aria-label="Permalink for this section"></a></h4><blockquote><p>Category: governance control<br>Permalink: <a href=https://owaspai.org/goto/secprogram/ target=_blank rel=noopener>https://owaspai.org/goto/secprogram/</a></p></blockquote><p><strong>Description</strong><br>Security Program: Make sure the organization has a security program (also referred to as <em>information security management system</em>) and that it includes the whole AI lifecycle and AI specific aspects.</p><p><strong>Objective</strong><br>Ensures adequate mitigation of AI security risks through information security management, as the security program takes responsibility for the AI-specific threats and corresponding risks. For more details on using this document in risk analysis, see the <a href=/goto/riskanalysis/>risk analysis section</a>.</p><p><strong>Implementation</strong><br>Make sure to include AI-specific assets and the threats to them. The threats are covered in this resource and the assets are:</p><ul><li>training data</li><li>validation data</li><li>test data</li><li>the model - often referred to as <em>model parameters</em> (values that change when a model is trained)</li><li>hyperparameters</li><li>documentation of models and the process of their development including experiments</li><li>model input</li><li>model output, which needs to be regarded as untrusted if the training data or model is untrusted</li><li>intended model behaviour</li><li>data to train and test obtained from external sources</li><li>models to train and use from external sources</li><li>augmentation data that is inserted into the model input</li></ul><p>By incorporating these assets and the threats to them, the security program takes care of mitigating these risks. For example: by informing engineers in awareness training that they should not leave their documentation lying around. Or: by installing malware detection on engineer machines because of the high sensitivity of the training data that they work with.</p><p>Every AI initiative, new and existing, should perform a privacy and security risk analysis. AI programs have additional concerns around privacy and security that need to be considered. While each system implementation will be different based on its contextual purpose, the same process can be applied. These analyses can be performed before the development process and will guide security and privacy controls for the system. These controls are based on security protection goals such as Confidentiality, Integrity and Availability, and privacy goals such as Unlinkability, Transparency and Intervenability. ISO/IEC TR 27562:2023 provides a detailed list of points of attention for these goals and coverage.</p><p>The general process for performing an AI Use Case Privacy and Security Analysis is:</p><ul><li>Describe the Ecosystem</li><li>Provide an assessment of the system of interest</li><li>Identify the security and privacy concerns</li><li>Identify the security and privacy risks</li><li>Identify the security and privacy controls</li><li>Identify the security and privacy assurance concerns</li></ul><p>Because AI has specific assets (e.g. training data), <strong>AI-specific honeypots</strong> are a particularly interesting control. These are fake parts of the data/model/data science infrastructure that are exposed on purpose, in order to detect or capture attackers, before they succeed to access the real assets. Examples:</p><ul><li>Hardened data services, but with an unpatched vulnerability (e.g. Elasticsearch)</li><li>Exposed data lakes, not revealing details of the actual assets</li><li>Data access APIs vulnerable to brute-force attacks</li><li>&ldquo;Mirror&rdquo; data servers that resemble development facilities, but are exposed in production with SSH access and labeled with names like &ldquo;lab&rdquo;</li><li>Documentation &lsquo;accidentally&rsquo; exposed, directing to a honeypot</li><li>Data science Python library exposed on the server</li><li>External access granted to a specific library</li><li>Models imported as-is from GitHub</li></ul><p>Monitoring and incident response are standard elements of security programs and AI can be included in it by understanding the relevant AI security assets, threats, and controls. The discussion of threats include detection mechanisms that become part of monitoring.</p><p><strong>References</strong><br>Useful standards include:</p><ul><li><p>The entire ISO 27000-27005 range is applicable to AI systems in the general sense as they are IT systems. Gap: covers this control fully regarding the processes, with the high-level particularity that there are three AI-specific attack surfaces that need to be taken into account in information security management: 1)AI development-time attacks, 2)attacks through model use and 3)AI Application security attacks. See the controls under the corresponding sections to see more particularities.
These standards cover:</p><ul><li>ISO/IEC 27000 – Information security management systems – Overview and vocabulary</li><li>ISO/IEC 27001 – Information security management systems – Requirements</li><li>ISO/IEC 27002 – Code of practice for information security management (See below)</li><li>ISO/IEC 27003 – Information security management systems: Implementation Guidelines</li><li>ISO/IEC 27004 – Information security management measurements</li><li>ISO/IEC 27005 – Information security risk management</li></ul></li><li><p>The &lsquo;27002 controls&rsquo; mentioned throughout this document are listed in the Annex of ISO 27001, and further detailed with practices in ISO 27002. At the high abstraction level, the most relevant ISO 27002 controls are:</p><ul><li>ISO 27002 control 5.1 Policies for information security</li><li>ISO 27002 control 5.10 Acceptable use of information and other associated assets</li><li>ISO 27002 control 5.8 Information security in project management</li></ul></li><li><p><a href=https://www.opencre.org/cre/261-010 target=_blank rel=noopener>OpenCRE on security program management</a></p></li><li><p>Risk analysis standards:</p><ul><li>This document contains AI security threats and controls to facilitate risk analysis</li><li>See also <a href=https://atlas.mitre.org/ target=_blank rel=noopener>MITRE ATLAS framework for AI threats</a></li><li>ISO/IEC 27005 - as mentioned above. Gap: covers this control fully, with said particularity (as ISO 27005 doesn&rsquo;t mention AI-specific threats)</li><li>ISO/IEC 27563:2023 (AI use cases security & privacy) Discusses the impact of security and privacy in AI use cases and may serve as useful input to AI security risk analysis. The work bases its list of AI use cases on the 132 use cases belonging to 22 application domains in ISO/IEC TR 24030:2021, identifies 11 use cases with a maximum concern rating for security and 49 use cases with a maximum concern rating for privacy.</li><li>ISO/IEC 23894 (AI Risk management). Gap: covers this control fully - It refers to ISO/IEC 24028 (AI trustworthiness) for AI security threats. However, ISO/IEC 24028 is not as comprehensive as AI Exchange (this document) or MITRE ATLAS as it is focused on risk management rather than threat enumeration.</li><li>ISO/IEC 5338 (AI lifecycle) covers the AI risk management process. Gap: same as ISO 23894 above.</li><li><a href=https://www.etsi.org/deliver/etsi_ts/102100_102199/10216501/05.02.03_60/ts_10216501v050203p.pdf target=_blank rel=noopener>ETSI Method and pro forma for Threat, Vulnerability, Risk Analysis</a></li><li><a href=https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf target=_blank rel=noopener>NIST AI Risk Management Framework</a></li><li><a href=https://www.opencre.org/cre/307-242 target=_blank rel=noopener>OpenCRE on security risk analysis</a></li><li><a href=https://csrc.nist.gov/pubs/sp/800/53/r5/upd1/final target=_blank rel=noopener>NIST SP 800-53 on general security/privacy controls</a></li><li><a href=https://www.nist.gov/cyberframework target=_blank rel=noopener>NIST cyber security framework</a></li><li><a href=https://genai.owasp.org/resource/llm-and-generative-ai-security-center-of-excellence-guide/ target=_blank rel=noopener>GenAI security project LLM and GenAI Security Center of Excellence guide</a></li></ul></li></ul><h4>#SEC DEV PROGRAM<span class="absolute -mt-20" id=sec-dev-program></span>
<a href=#sec-dev-program class=subheading-anchor aria-label="Permalink for this section"></a></h4><blockquote><p>Category: governance control<br>Permalink: <a href=https://owaspai.org/goto/secdevprogram/ target=_blank rel=noopener>https://owaspai.org/goto/secdevprogram/</a></p></blockquote><p><strong>Description</strong><br>Secure development program: Have processes concerning software development in place to make sure that security is built into your AI system.</p><p><strong>Objective</strong><br>Reduces security risks by proper attention to mitigating those risks during software development.</p><p><strong>Implementation</strong><br>The best way to do this is to build on your existing secure software development practices and include AI teams and AI particularities. This means that data science development activities should become part of your secure software development practices. Examples of these practices: secure development training, code review, security requirements, secure coding guidelines, threat modeling (including AI-specific threats), static analysis tooling, dynamic analysis tooling, and penetration testing. There is no need for an isolated secure development framework for AI.</p><p><strong>Particularity</strong><br>Particularities for AI in software development, and how to address them:</p><ul><li><p>AI involves new types of engineering: data engineering and model engineering (e.g. model training), together with new types of engineers: e.g. data scientists, data engineers, AI engineers. Make sure this new engineering becomes an integral part of the general <a href=/goto/devprogram/>Development program</a> with its best practices (e.g. versioning, portfolio management, retirement). For example: Version management/traceability of the combination of code, configuration, training data and models, for troubleshooting and rollback</p></li><li><p>New assets, threats and controls (as covered in this document) need to be considered, affecting requirements, policies, coding guidelines, training, tooling, testing practices and more. Usually, this is done by adding these elements in the organization&rsquo;s Information Security Management System, as described in <a href=/goto/secprogram/>SECPROGRAM</a>, and align secure software development to that - just like it has been aligned on the conventional assets, threats and controls (see <a href=/goto/secdevprogram/>SECDEVPROGRAM</a>). This involves both conventional security threats and AI-specific threats, applying both conventional security controls and AI-specific ones. Typically, technical teams depend on the AI engineers when it comes to the AI-specific controls as they mostly require deep AI expertise. For example: if training data is confidential and collected in a distributed way, then a federated learning approach may be considered.</p></li><li><p>Apart from software components, the supply chain for AI can also include data and models which may have been poisoned, which is why data provenance and model management are central in <a href=/goto/supplychainmanage/>AI supply chain management</a>.</p></li><li><p>In AI, software components can also run in the development, for example tools to prepare training data or train a model. Because of this, the AI development environment is vulnerable to traditional software security risks, such as open source package vulnerabilities, CWEs, exposed secrets, and sensitive data leaks. Without robust controls in place, these risks go undetected by standard application security testing tools, potentially exposing the entire lifecycle to breaches.</p></li><li><p>The AI development environment typically involves sensitive data, in contrast to conventional engineering where the use of such data by engineers is normally avoided. Therefore, apply <a href=/goto/devsecurity/>development security</a> on the development environment. In addition to the conventional assets of code, configuration and secrets, the AI-specific development assets are:</p><ul><li>Potentially sensitive data needed to train, test and validate models</li><li>Model parameters, which often represent intellectual property and can also be used to prepare input attacks when obtained.</li></ul></li><li><p>New best practices or pitfalls in AI-specific code:</p><ul><li>Run static analysis rules specific to big data/AI technology (e.g., the typical mistake of creating a new dataframe in Python without assigning it to a new one)</li><li>Run maintainability analysis on code, as data and model engineering code is typically hindered by code quality issues</li><li>Evaluate code for the percentage of code for automated testing. Industry average is 43% (SIG benchmark report 2023). An often cited recommendation is 80%. Research shows that automated testing in AI engineering is often neglected (SIG benchmark report 2023), as the performance of the AI model is mistakenly regarded as the ground truth of correctness.</li></ul></li><li><p>Model performance testing is essential</p><ul><li>Run AI-specific dynamic performance tests before deployment (see <a href=/goto/continuousvalidation/>#CONTINUOUS VALIDATION</a>):</li><li>Run security tests (e.g. data poisoning payloads, prompt injection payloads, adversarial robustness testing). See the <a href=/goto/testing/>testing section</a>.</li><li>Run continual automated validation of the model, including discrimination bias measurement and the detection of staleness: the input space changing over time, causing the training set to get out of date</li></ul></li><li><p>Model deployment is a new aspect to AI and it may offer specific protection measures such as obfuscation, encryption, integrity checks or a Trusted Execution Environment.</p></li></ul><p><strong>Risk-Reduction guidance</strong><br>Depending on risk analysis, certain threats may require specific practices in the development lifecycle. These threats and controls are covered elsewhere in this document.</p><p><strong>References</strong></p><ul><li><a href=https://owaspsamm.org target=_blank rel=noopener>OWASP SAMM</a></li><li><a href=https://csrc.nist.gov/projects/ssdf target=_blank rel=noopener>NIST SSDF</a></li><li><a href=https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-218A.ipd.pdf target=_blank rel=noopener>NIST SSDF AI practices</a></li><li><a href=https://genai.owasp.org/ai-security-solutions-landscape/ target=_blank rel=noopener>GenAI security project solutions overview</a></li></ul><p>Useful standards include:</p><ul><li>ISO 27002 control 8.25 Secure development lifecycle. Gap: covers this control fully, with said particularity, but lack of detail - the 8.25 Control description in ISO 27002:2022 is one page, whereas secure software development is a large and complex topic - see below for further references</li><li>ISO/IEC 27115 (Cybersecurity evaluation of complex systems)</li><li>See <a href=https://www.opencre.org/cre/616-305 target=_blank rel=noopener>OpenCRE on secure software development processes</a> with notable links to NIST SSDF and OWASP SAMM. Gap: covers this control fully, with said particularity</li></ul><h4>#DEV PROGRAM<span class="absolute -mt-20" id=dev-program></span>
<a href=#dev-program class=subheading-anchor aria-label="Permalink for this section"></a></h4><blockquote><p>Category: governance control<br>Permalink: <a href=https://owaspai.org/goto/devprogram/ target=_blank rel=noopener>https://owaspai.org/goto/devprogram/</a></p></blockquote><p><strong>Description</strong><br>Development program: Having a development lifecycle program for AI. Apply general (not just security-oriented) software engineering best practices to AI development.</p><p>Data scientists are focused on creating working models, not on creating future-proof software per se. Often, organizations already have software practices and processes in place. It is important to extend these to AI development, instead of treating AI as something that requires a separate approach. Do not isolate AI engineering. This includes automated testing, code quality, documentation, and versioning. ISO/IEC 5338 explains how to make these practices work for AI.</p><p><strong>Objective</strong><br>This way, AI systems will become easier to maintain, transferable, secure, more reliable, and future-proof.</p><p><strong>Implementation</strong><br>A best practice is to mix data scientist profiles with software engineering profiles in teams, as software engineers typically need to learn more about data science, and data scientists generally need to learn more about creating future-proof, maintainable, and easily testable code.</p><p>Another best practice is to continuously measure quality aspects of data science code (maintainability, test code coverage), and provide coaching to data scientists in how to manage those quality levels.</p><p>Apart from conventional software best practices, there are important AI-specific engineering practices, including for example data provenance & lineage, model traceability and AI-specific testing such as continuous validation, testing for model staleness and concept drift. ISO/IEC 5338 discusses these AI engineering practices.</p><p>Related controls that are key parts of the development lifecycle:</p><ul><li><a href=/goto/secdevprogram/>Secure development program</a></li><li><a href=/goto/supplychainmanage/>Supply chain management</a></li><li><a href=/goto/continuousvalidation/>Continuous validation</a></li><li><a href=/goto/unwantedbiastesting/>Unwanted bias testing</a></li></ul><p>The below interpretation diagram of ISO/IEC 5338 provides a good overview to get an idea of the topics involved.
<img src=/images/5338.png alt=5338 loading=lazy></p><p><strong>References</strong></p><ul><li><a href=https://www.softwareimprovementgroup.com/averting-a-major-ai-crisis-we-need-to-fix-the-big-quality-gap-in-ai-systems/ target=_blank rel=noopener>Research on code quality gaps in AI systems</a></li></ul><p>Useful standards include:</p><ul><li><a href=https://www.iso.org/standard/81118.html target=_blank rel=noopener>ISO/IEC 5338 - AI lifecycle</a> Gap: covers this control fully - ISO 5338 covers the complete software development lifecycle for AI, by extending the existing ISO 12207 standard on software lifecycle: defining several new processes and discussing AI-specific particularities for existing processes. See also <a href=https://www.softwareimprovementgroup.com/iso-5338-get-to-know-the-global-standard-on-ai-systems/ target=_blank rel=noopener>this blog</a>.</li><li><a href=https://www.iso.org/standard/75652.html target=_blank rel=noopener>ISO/IEC 27002</a> control 5.37 Documented operating procedures. Gap: covers this control minimally - this covers only a very small part of the control</li><li><a href=https://www.opencre.org/cre/162-655 target=_blank rel=noopener>OpenCRE on documentation of function</a> Gap: covers this control minimally</li></ul><h4>#CHECK COMPLIANCE<span class="absolute -mt-20" id=check-compliance></span>
<a href=#check-compliance class=subheading-anchor aria-label="Permalink for this section"></a></h4><blockquote><p>Category: governance control<br>Permalink: <a href=https://owaspai.org/goto/checkcompliance/ target=_blank rel=noopener>https://owaspai.org/goto/checkcompliance/</a></p></blockquote><p><strong>Description</strong><br>Check compliance: Make sure that AI-relevant laws and regulations are taken into account in compliance management (including security aspects). If personal data is involved and/or AI is applied to make decisions about individuals, then privacy laws and regulations are also in scope. See the <a href=/goto/aiprivacy/>Privacy section</a> for details.</p><p><strong>Objective</strong><br>Compliance as a goal can be a powerful driver for organizations to grow their readiness for AI. While doing this, it is important to keep in mind that legislation has a scope that does not necessarily include all the relevant risks for the organization. Many rules are about the potential harm to individuals and society, and don’t cover the impact on business stakes per se. For example: the European AI act does not include risks for protecting company secrets. In other words: be mindful of blind spots when using laws and regulations as your guide.</p><p>Global Jurisdictional considerations (as of end of 2023):</p><ul><li>Canada: Artificial Intelligence & Data Act</li><li>USA: (i) Federal AI Disclosure Act, (ii) Federal Algorithmic Accountability Act</li><li>Brazil: AI Regulatory Framework</li><li>India: Digital India Act</li><li>Europe: (i) AI Act, (ii) AI Liability Directive, (iii) Product Liability Directive</li><li>China: (i) Regulations on the Administration of Deep Synthesis of Internet Information Services, (ii) Shanghai Municipal Regulations on Promoting Development of AI Industry, (iii) Shenzhen Special Economic Zone AI Industry Promotion Regulations, (iv) Provisional Administrative Measures for Generative AI Services</li></ul><p><strong>Implementation</strong><br>General Legal Considerations on AI/Security:</p><ul><li>Privacy Laws: AI must comply with all local/global privacy laws at all times, such as GDPR, CCPA, HIPAA. See the <a href=/goto/aiprivacy/>Privacy section</a>.</li><li>Data Governance: any AI components/functions provided by a 3rd party for integration must have data governance frameworks, including those for the protection of personal data and structure/definitions on how its collected, processed, stored</li><li>Data Breaches: any 3rd party supplier must answer as to how they store their data and security frameworks around it, which may include personal data or IP of end-users</li></ul><p>Non-Security Compliance Considerations:</p><ul><li>Ethics: Deep fake weaponization and how the system addresses and deals with it, protects against it and mitigates it</li><li>Human Control: any and all AI systems should be deployed with appropriate levels of human control and oversight, based on ascertained risks to individuals. AI systems should be designed and utilized with the concept that the use of AI respects dignity and rights of individuals; “Keep the human in the loop” concept. See <a href=/goto/oversight/>Oversight</a>.</li><li>Discrimination: a process must be included to review datasets to avoid and prevent any bias. See <a href=/goto/unwantedbiastesting/>Unwanted bias testing</a>.</li><li>Transparency: ensure transparency in the AI system deployment, usage and proactive compliance with regulatory requirements; “Trust by Design”</li><li>Accountability: AI systems should be accountable for actions and outputs and usage of data sets. See <a href=/goto/aiprogram/>AI Program</a></li></ul><p><strong>References</strong></p><ul><li><a href=https://www.vischer.com/en/artificial-intelligence/ target=_blank rel=noopener>Vischer on legal aspects of AI</a></li><li><a href=https://www.softwareimprovementgroup.com/eu-ai-act-summary/ target=_blank rel=noopener>Summary of AI Act by SIG</a></li><li><a href=https://www.softwareimprovementgroup.com/us-ai-legislation-overview/ target=_blank rel=noopener>Summary of US AI legislation by SIG</a></li></ul><p>Useful standards include:</p><ul><li><a href=https://www.opencre.org/cre/510-324 target=_blank rel=noopener>OpenCRE on Compliance</a></li><li>ISO 27002 Control 5.36 Compliance with policies, rules and standards. Gap: covers this control fully, with the particularity that AI regulation needs to be taken into account.</li><li>ISO/IEC 27090 (AI security) and 27091 (AI privacy) are both in development at the moment of writing (Oct 2025), and likely come out in 2026. The AI Exchange has contributed substantial content to the 27090.</li><li>prEN 18282 is the European standard for AI Security - brought forward by CEN/CENELEC and with a substantial part contributed by the AI Exchange. Exchange founder Rob van der Veer is liaison officer for the official partnership between the AI Exchange and CEN/CENELEC/ISO, as well as co-editor for 18282. The standard has been in development for almost two years at the moment of writing (Oct 2025) and expected to go into public enquiry early 2026, and be published in 2026.</li></ul><h4>#SEC EDUCATE<span class="absolute -mt-20" id=sec-educate></span>
<a href=#sec-educate class=subheading-anchor aria-label="Permalink for this section"></a></h4><blockquote><p>Category: governance control<br>Permalink: <a href=https://owaspai.org/goto/seceducate/ target=_blank rel=noopener>https://owaspai.org/goto/seceducate/</a></p></blockquote><p><strong>Description</strong><br>Security education for data scientists and development teams on AI threat awareness, including attacks on models. It is essential for all engineers, including data scientists, to attain a security mindset.</p><p><strong>References</strong><br>Useful standards include:</p><ul><li>ISO 27002 Control 6.3 Awareness training. Gap: covers this control fully, but lacks detail and needs to take into account the particularity: training material needs to cover AI security threats and controls</li></ul><hr><h2>1.2 General controls for sensitive data limitation<span class="absolute -mt-20" id=12-general-controls-for-sensitive-data-limitation></span>
<a href=#12-general-controls-for-sensitive-data-limitation class=subheading-anchor aria-label="Permalink for this section"></a></h2><blockquote><p>Category: group of controls<br>Permalink: <a href=https://owaspai.org/goto/datalimit/ target=_blank rel=noopener>https://owaspai.org/goto/datalimit/</a></p></blockquote><p>The impact of security threats on confidentiality and integrity can be reduced by limiting the data attack surface, meaning that the amount and the variety of data is reduced as much as possible, as well as the duration in which it is kept. This section describes several controls to apply this limitation.</p><h4>#DATA MINIMIZE<span class="absolute -mt-20" id=data-minimize></span>
<a href=#data-minimize class=subheading-anchor aria-label="Permalink for this section"></a></h4><blockquote><p>Category: development-time and runtime control<br>Permalink: <a href=https://owaspai.org/goto/dataminimize/ target=_blank rel=noopener>https://owaspai.org/goto/dataminimize/</a></p></blockquote><p><strong>Description</strong><br>Data minimize: remove data fields or records (e.g. from a training set) that are unnecessary for the application, in order to prevent potential data leaks or manipulation because we cannot leak what isn’t there in the first place</p><p><strong>Objective</strong><br>Minimize the impact of data leakage or manipulation by reducing the amount of data processed by the system.</p><p><strong>Applicability</strong><br>Data minimization applies during data collection, preparation, training, evaluation, and runtime logging. It is particularly relevant when datasets contain personal, confidential, or exposure-restricted information.
An exception to applicability to the AI system provider is when the deployer is better positioned to implement (part of) this control, as long as the provider communicates this requirement to the deployer.</p><p><strong>Implementation</strong><br>In addition to removing (or archiving) unused or low-impact fields and records, data minimization can include:</p><ul><li>removing data elements (fields, record) that do not materially affect model performance (e.g. correctness, robustness, fairness) based on experimentation or analysis;</li><li>retaining certain identifiers only to support data removal requests or lifecycle management, while excluding them from model training;</li><li>updating training datasets to reflect removals or corrections made in upstream source data (e.g. when personal data is destroyed from the source data then training data is updated to reflect the change);</li><li>original data can be preserved separately with access controls for future use.</li></ul><p><strong>Risk-Reduction Guidance</strong><br>Data minimization reduces confidentiality risk by limiting the presence of exposure-restricted information. Data that is not collected or retained cannot be leaked, reconstructed, or inferred from the system. It also reduces the consequences of dataset theft or unauthorized access.</p><p><strong>Particularity</strong><br>AI models often tolerate reduced feature sets and incomplete data better than traditional applications, enabling stronger minimization strategies without functional loss.</p><p><strong>References</strong><br>Useful standards include:</p><ul><li>Not covered yet in ISO/IEC standards.</li></ul><h4>#ALLOWED DATA<span class="absolute -mt-20" id=allowed-data></span>
<a href=#allowed-data class=subheading-anchor aria-label="Permalink for this section"></a></h4><blockquote><p>Category: development-time and runtime control<br>Permalink: <a href=https://owaspai.org/goto/alloweddata/ target=_blank rel=noopener>https://owaspai.org/goto/alloweddata/</a></p></blockquote><p><strong>Description</strong><br>Ensure allowed data, meaning: removing data (e.g. from a training set) that is prohibited for the intended purpose. This is particularly important if consent was not given and the data contains personal information collected for a different purpose.</p><p><strong>Objective</strong><br>Apart from compliance, the purpose is to minimize the impact of data leakage or manipulation</p><p><strong>References</strong><br>Useful standards include:</p><ul><li>ISO/IEC 23894 (AI risk management) covers this in A.8 Privacy. Gap: covers this control fully, with a brief section on the idea</li></ul><h4>#SHORT RETAIN<span class="absolute -mt-20" id=short-retain></span>
<a href=#short-retain class=subheading-anchor aria-label="Permalink for this section"></a></h4><blockquote><p>Category: development-time and runtime control<br>Permalink: <a href=https://owaspai.org/goto/shortretain/ target=_blank rel=noopener>https://owaspai.org/goto/shortretain/</a></p></blockquote><p><strong>Description</strong><br>Short retain: Remove or anonymize data once it is no longer needed, or when legally required (e.g., due to privacy laws).</p><p><strong>Objective</strong><br>Minimize the impact of data leakage or manipulation</p><p><strong>Implementation</strong><br>Limiting the retention period of data can be seen as a special form of data minimization. Privacy regulations typically require personal data to be removed when it is no longer needed for the purpose for which it was collected. Sometimes exceptions need to be made because of other rules (e.g. to keep a record of proof). Apart from these regulations, it is a general best practice to remove any sensitive data when it is no longer of use, to reduce the impact of a data leak.</p><p><strong>References</strong><br>Useful standards include:</p><ul><li>Not covered yet in ISO/IEC standards.</li></ul><h4>#OBFUSCATE TRAINING DATA<span class="absolute -mt-20" id=obfuscate-training-data></span>
<a href=#obfuscate-training-data class=subheading-anchor aria-label="Permalink for this section"></a></h4><blockquote><p>Category: development-time AI engineer control<br>Permalink: <a href=https://owaspai.org/goto/obfuscatetrainingdata/ target=_blank rel=noopener>https://owaspai.org/goto/obfuscatetrainingdata/</a></p></blockquote><p><strong>Description</strong><br>Obfuscate training data: attain a degree of obfuscation of sensitive data where possible.</p><p><strong>Objective</strong><br>Minimize the impact of data leakage or manipulation when sensitive data cannot be removed entirely, by making the data less recognizable or harder to reconstruct.</p><p><strong>Applicability</strong><br>Data obfuscation is particularly relevant when exposure-restricted data is necessary for training, compliance, or risk mitigation, and cannot be removed.
An exception to applicability to the AI system provider is when the deployer is better positioned to implement this control, as long as the provider communicates this requirement to the deployer.</p><p><strong>Implementation</strong><br>Obfuscation techniques include:</p><ul><li><p><strong>Private Aggregation of Teacher Ensembles (PATE)</strong><br>Private Aggregation of Teacher Ensembles (PATE) is a privacy-preserving machine learning technique. This method tackles the challenge of training models on sensitive data while maintaining privacy. It achieves this by employing an ensemble of &ldquo;teacher&rdquo; models along with a &ldquo;student&rdquo; model. Each teacher model is independently trained on distinct subsets of sensitive data, ensuring that there is no overlap in the training data between any pair of teachers. Since no single model sees the entire dataset, it reduces the risk of exposing sensitive information. Once the teacher models are trained, they are used to make predictions. When a new (unseen) data point is presented, each teacher model gives its prediction. These predictions are then aggregated to reach a consensus. This consensus is considered more reliable and less prone to individual biases or overfitting to their respective training subsets. To further enhance privacy, noise is added to the aggregated predictions. By adding noise, the method ensures that the final output doesn&rsquo;t reveal specifics about the training data of any individual teacher model. The student model is trained not on the original sensitive data, but on the aggregated and noised predictions of the teacher models. Essentially, the student learns from the collective wisdom and privacy-preserving outputs of the teachers. This way, the student model can make accurate predictions without ever directly accessing the sensitive data. However, there are challenges in balancing the amount of noise (for privacy) and the accuracy of the student model. Too much noise can degrade the performance of the student model, while too little might compromise privacy.</p></li><li><p><strong>Objective function perturbation</strong>
Objective function perturbation is a differential privacy technique used to train machine learning models while maintaining data privacy. It involves the intentional introduction of a controlled amount of noise into the learning algorithm’s objective function, which is a measure of the discrepancy between a model’s predictions and the actual results. The perturbation, or slight modification, involves adding noise to the objective function, resulting in a final model that doesn’t exactly fit the original data, thereby preserving privacy. The added noise is typically calibrated to the objective function’s sensitivity to individual data points and the desired privacy level, as quantified by parameters like epsilon in differential privacy. This ensures that the trained model doesn’t reveal sensitive information about any individual data point in the training dataset. The main challenge in objective function perturbation is balancing data privacy with the accuracy of the resulting model. Increasing the noise enhances privacy but can degrade the model’s accuracy. The goal is to strike an optimal balance where the model remains useful while individual data points stay private.</p></li><li><p><strong>Masking</strong><br>Masking involves the alteration or replacement of sensitive features within datasets with alternative representations that retain the essential information required for training while obscuring sensitive details. Various methods can be employed for masking, including tokenization, perturbation, generalization, and feature engineering. Tokenization replaces sensitive text data with unique identifiers, while perturbation adds random noise to numerical data to obscure individual values. Generalization involves grouping individuals into broader categories, and feature engineering creates derived features that convey relevant information without revealing sensitive details. Once the sensitive features are masked or transformed, machine learning models can be trained on the modified dataset, ensuring that they learn useful patterns without exposing sensitive information about individuals. However, achieving a balance between preserving privacy and maintaining model utility is crucial, as more aggressive masking techniques may lead to reduced model performance.</p></li><li><p><strong>Encryption</strong><br>Encryption is a fundamental technique for pseudonymization and data protection. It underscores the need for careful implementation of encryption techniques, particularly asymmetric encryption, to achieve robust pseudonymization. Emphasis is placed on the importance of employing randomized encryption schemes, such as Paillier and Elgamal, to ensure unpredictable pseudonyms. Furthermore, homomorphic encryption, which allows computations on ciphertexts without the decryption key, presents potential advantages for cryptographic operations but poses challenges in pseudonymization. The use of asymmetric encryption for outsourcing pseudonymization and the introduction of cryptographic primitives like ring signatures and group pseudonyms in advanced pseudonymization schemes are important.<br>There are two models of encryption in machine learning:</p><ol><li>(part of) the data remains in encrypted form for the data scientists all the time, and is only in its original form for a separate group of data engineers that prepare and then encrypt the data for the data scientists.</li><li>The data is stored and communicated in encrypted form to protect against access from users outside the data scientists, but is used in its original form when analysed, and transformed by the data scientists and the model. In the second model it is important to combine the encryption with proper access control, because it hardly offers protection to encrypt data in a database and then allow any user access to that data through the database application.</li></ol></li><li><p><strong>Tokenization</strong><br>Tokenization is a technique for obfuscating data with the aim of enhancing privacy and security in the training of machine learning models. The objective is to introduce a level of obfuscation to sensitive data, thereby reducing the risk of exposing individual details while maintaining the data&rsquo;s utility for model training. In the process of tokenization, sensitive information, such as words or numerical values, is replaced with unique tokens or identifiers. This substitution makes it difficult for unauthorized users to derive meaningful information from the tokenized data.<br>Within the realm of personal data protection, tokenization aligns with the principles of differential privacy. When applied to personal information, this technique ensures that individual records remain indiscernible within the training data, thus safeguarding privacy. Differential privacy involves introducing controlled noise or perturbations to the data to prevent the extraction of specific details about any individual.<br>Tokenization aligns with this concept by replacing personal details with tokens, increasing the difficulty of linking specific records back to individuals.
Tokenization proves particularly advantageous in development-time data science when handling sensitive datasets. It enhances security by enabling data scientists to work with valuable information without compromising individual privacy. The implementation of tokenization techniques supports the broader objective of obfuscating training data, striking a balance between leveraging valuable data insights and safeguarding the privacy of individuals.</p></li></ul><p><strong>Risk-Reduction Guidance</strong><br>Obfuscation reduces the likelihood that training data can be reconstructed or linked back to individuals. Effectiveness can be evaluated through attack testing or by relying on formal privacy guarantees such as differential privacy or an equivalent mathematical framework. Residual risk remains when exposure-restricted data is still present, when obfuscation mechanisms fail, or when reconstruction or re-identification remains possible, such as through access to token mapping tables.</p><p><strong>Particularity</strong><br>AI models typically do not require exact or human-readable representations of training data, allowing obfuscation techniques that would be impractical in traditional systems. In traditional systems, data attributes are processed directly leaving less room for obfuscation techniques.</p><p><strong>Limitations</strong><br>Obfuscation reduces the risk of re-identification or inference, but does not eliminate it:</p><ul><li>Removing or obfuscating PII / personal data is often not sufficient, as someone&rsquo;s identity may be induced from the other data that you keep of the person (locations, times, visited websites, activities together with data and time, etc.).</li><li>Token-based approaches introduce additional risk if mapping tables are compromised.</li></ul><p>The risk of re-identification can be assessed by experts using statistical properties such as K-anonymity, L-diversity, and T-closeness.<br>Anonymity is not an absolute concept, but a statistical one. Even if someone&rsquo;s identity can be guessed from data with some certainty, it can be harmful. The concept of <em>differential privacy</em> helps to analyse the level of anonymity. It is a framework for formalizing privacy in statistical and data analysis, ensuring that the privacy of individual data entries in a database is protected. The key idea is to make it possible to learn about the population as a whole while providing strong guarantees that the presence or absence of any single individual in the dataset does not significantly affect the outcome of any analysis. This is often achieved by adding a controlled amount of random noise to the results of queries on the database. This noise is carefully calibrated to mask the contribution of individual data points, which means that the output of a data analysis (or query) should be essentially the same, whether any individual&rsquo;s data is included in the dataset or not. In other words by observing the output, one should not be able to infer whether any specific individual&rsquo;s data was used in the computation.</p><p>Distorting training data can make it effectively uncrecognizable, which of course needs to be weighed against the negative effect on model performance that this typically creates. See also <a href=/goto/traindatadistortion/>TRAINDATADISTORTION</a> which is about distortion against data poisoning and <a href=/goto/evasionrobustmodel/>EVASIONROBUSTMODEL</a> for distortion against evasion attacks. Together with this control OBFUSCATETRAININGDATA, these are all approaches that distort training data, but for different purposes.</p><p><strong>References</strong></p><ul><li><a href=https://arxiv.org/abs/2204.05157 target=_blank rel=noopener>SF-PATE: Scalable, Fair, and Private Aggregation of Teacher Ensembles</a></li><li><a href=https://arxiv.org/abs/1909.01783v1 target=_blank rel=noopener>Differentially Private Objective Perturbation: Beyond Smoothness and Convexity</a></li><li><a href=https://arxiv.org/abs/1901.02185 target=_blank rel=noopener>Data Masking with Privacy Guarantees</a></li><li>Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B., Mironov, I., Talwar, K., & Zhang, L. (2016). Deep learning with differential privacy. Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security, 308-318. <a href=https://doi.org/10.1145/2976749.2978318 target=_blank rel=noopener>Link</a></li><li>Dwork, C., & Roth, A. (2014). The Algorithmic Foundations of Differential Privacy. Foundations and Trends in Theoretical Computer Science. <a href=https://doi.org/10.1561/0400000042 target=_blank rel=noopener>Link</a></li></ul><p>Useful standards include:</p><ul><li>Not covered yet in ISO/IEC standards.</li></ul><h4>#DISCRETE<span class="absolute -mt-20" id=discrete></span>
<a href=#discrete class=subheading-anchor aria-label="Permalink for this section"></a></h4><blockquote><p>Category: development-time and runtime control<br>Permalink: <a href=https://owaspai.org/goto/discrete/ target=_blank rel=noopener>https://owaspai.org/goto/discrete/</a></p></blockquote><p><strong>Description</strong><br>Minimize access to technical details that could help attackers.</p><p><strong>Objective</strong><br>Reduce the information available to attackers, which can assist them in selecting and tailoring their attacks, thereby lowering the probability of a successful attack.</p><p><strong>Implementation</strong><br>Minimizing and protecting technical details can be achieved by incorporating such details as an asset into information security management. This will ensure proper asset management, data classification, awareness education, policy, and inclusion in risk analysis.</p><p>Note: this control needs to be weighed against the <a href=/goto/aitransparency/>#AI TRANSPARENCY</a> control that nay require to be more open about technical aspects of the model. The key is to minimize information that can help attackers while being transparent.</p><p>For example:</p><ul><li>Consider this risk when publishing technical articles on the AI system</li><li>When choosing a model type or model implementation, take into account that there is an advantage of having technology with which attackers are less familiar</li><li>Minimize technical details in model output</li></ul><p><strong>References</strong><br>Useful standards include:</p><ul><li>ISO 27002 Control 5.9: Inventory of information and other associated assets. Gap: covers this control fully, with the particularity that technical data science details can be sensitive. .</li><li>See <a href=https://www.opencre.org/cre/074-873 target=_blank rel=noopener>OpenCRE on data classification and handling</a>. Gap: idem</li><li><a href=https://atlas.mitre.org/techniques/AML.T0002 target=_blank rel=noopener>MITRE ATlAS Acquire Public ML Artifacts</a></li></ul><hr><h2>1.3. Controls to limit the effects of unwanted behaviour<span class="absolute -mt-20" id=13-controls-to-limit-the-effects-of-unwanted-behaviour></span>
<a href=#13-controls-to-limit-the-effects-of-unwanted-behaviour class=subheading-anchor aria-label="Permalink for this section"></a></h2><blockquote><p>Category: group of controls<br>Permalink: <a href=https://owaspai.org/goto/limitunwanted/ target=_blank rel=noopener>https://owaspai.org/goto/limitunwanted/</a></p></blockquote><p>Unwanted model behaviour is the intended result of many AI attacks (e.g. data poisoning, evasion, prompt injection). There are many ways to prevent and to detect these attacks, but this section is about how the effects of unwanted model behaviour can be controlled, in order to reduce the impact of an attack - by constraining actions, introducing oversight and enabling timely containment and recovery. This is sometimes referred to as <em>blast radius control</em>.</p><p>Besides attacks, AI systems can display unwanted behaviour for other reasons, making the control of this behaviour a shared responsibility, beyond just security. Key causes of unwanted model behaviour include:</p><ul><li>Insufficient or incorrect training data</li><li>Model staleness/ Model drift (i.e. the model becoming outdated)</li><li>Mistakes during model and data engineering</li><li>Feedback loops where model output ends up in the training data of future models, which leads to model collapse (also known as recursive pollution)</li><li>Security threats: attacks as laid out in this document</li></ul><p>Successfully mitigating unwanted model behaviour has its own threats:</p><ul><li>Overreliance: the model is being trusted too much by users</li><li>Excessive agency: the model is being trusted too much by engineers and gets excessive functionality, permissions, or autonomy</li></ul><p>Example: When Large Language Models (GenAI) can perform actions, the privileges around which actions and when become important (<a href=https://llmtop10.com/llm07/ target=_blank rel=noopener>OWASP for LLM 07</a>).</p><p>Example: LLMs (GenAI), just like most AI models, induce their results based on training data, meaning that they can make up things that are false. In addition, the training data can contain false or outdated information. At the same time, LLMs (GenAI) can come across as very confident about their output. These aspects make overreliance of LLM (GenAI) (<a href=https://llmtop10.com/llm09/ target=_blank rel=noopener>OWASP for LLM 09</a>) a real risk, plus excessive agency as a result of that (<a href=https://llmtop10.com/llm08/ target=_blank rel=noopener>OWASP for LLM 08</a>). Note that all AI models in principle can suffer from overreliance - not just Large Language Models.</p><p><strong>Controls to limit the effects of unwanted model behaviour:</strong></p><h4>#OVERSIGHT<span class="absolute -mt-20" id=oversight></span>
<a href=#oversight class=subheading-anchor aria-label="Permalink for this section"></a></h4><blockquote><p>Category: runtime control<br>Permalink: <a href=https://owaspai.org/goto/oversight/ target=_blank rel=noopener>https://owaspai.org/goto/oversight/</a></p></blockquote><p><strong>Description</strong><br>Oversight of model behaviour by humans or automated mechanisms (e.g.,using rules), where human oversight provides not only more intelligent validation through common sense and domain knowledge, but also clear accountability for devisions and outcomes.</p><p><strong>Objective</strong><br>Detect unwanted model behavior and respond to it. Responses include correcting, halting execution, deferring to an (other) human-in-the-loop, or issueing an alert to be investigated.</p><p><strong>Applicability</strong><br>It is the nature of AI models that they can be wrong. In addition, they can be manipulated (e.g., prompt injection, data poisoning, evasion), so it is critical to apply a layer of protection that oversees the output of the model. It is the final checkpoint.</p><p><strong>Implementation</strong></p><ul><li>Implement <strong>detection rules</strong> to recognize (potential) unwanted output, such as:<ul><li>Offensive language, toxicity, Not Safe For Work, misinformation, or dangerous information (e.g., recipe for poison, medical misinformation)</li><li>Sensitive data: see <a href=/goto/sensitiveoutputhandling/>SENSITIVE OUTPUT HANDLING</a> for the control to detect sensitive data (e.g. names, phone numbers, passwords, tokens). These detections can also be applied on the input of the model or on APIs that retrieve data to go into the model.</li><li>A special category of sensitive data: system prompts, as they can be used by attackers to circumvent prompt injection protection in such prompts.</li><li>Suspicious function calls. Ideally, the privileges of an AI model are already hardened to the task (see <a href=/goto/leastmodelprivilege/>#LEAST MODEL PRIVILEGE</a>), in which case detection comes down to issuing an alert once a model attempts to execute an action for which it has no permissions. In addition, the stategy can include the detection of unusual function calls in the context, issuing alerts for further investigation, or asking for approval by a human in the loop. Manipulation of function flow is commonly referred to as <em>application flow perturbation</em>. An advanced way to detect manipulated workflows is to perform rule-based sanity checks during steps, e.g. verify whether certain safety checks of filters were executed before processing data.</li></ul></li><li>Apply <strong>Grounding checks</strong> if recognizing unwanted output based on context is too difficult to catch in rules, and the detection of malicious input is insufficent. The idea of grounding checks is to let a separate Generative AI model decide if an input or output is off-topic or escalates capabilities (e.g. a LLM powered food recipes app suddenly is trying to send emails). This takes the use of LLMs to detect suspicious input and output a step further by including context. This is required in case GenAI-based recognition is insufficient to cover certain attack scenarios (see above).</li><li>Implement appropriate general detection and response mechanisms as presented in <a href=/goto/monitoruse/>#MONITOR USE</a> where part of the response can be to involve a human-in-the-loop.</li><li>Include as part of response options <strong>rollback mechanisms</strong> to enable oversight to go back to a certain state after system malfunction or manipulation has been observed and the state of the system cannot be trusted, or has been disrupted.</li><li>For checks that require accountability and/or more expertise and common sense, present the behaviour for a <strong>human</strong> to approve. This can be the result of a logic rule that in specific circumstances escalates to a human-in-the-loop.</li><li>Ensure that the <strong>human oversight is appropriate</strong>: the human is qualified, instructed, motivated, and not suffering from so-called <em>approval fatigue</em>: the result of having to approve many actions that are mostly in order.</li></ul><p>A separate form of oversight is <a href=/goto/modelalignment/>MODEL ALIGNMENT</a> which intends to constrain model behaviour through training, fine tuning, and system prompts. This is treated as a separate control because the effectiveness is limited and therefore no guarantee.</p><p>Examples:</p><ul><li>Logic preventing the trunk of a car from opening while the car is moving, even if the driver seems to request it</li><li>Logic signaling an alert when a software programming tool is making a series of updates to multiple projects in one go, after which the alert is processes by a human who can then decide to further investigate and/or to take action, which can include shutting down the complete system to prevent further harm</li><li>Requesting user confirmation before sending a large number of emails as instructed by a model</li><li>Another form of human oversight is allowing users to undo or revert actions initiated by the AI system, such as reversing changes made to a file</li><li>A special form of guardrails is censoring unwanted output of GenAI models (e.g. violent, unethical)</li></ul><p><strong>Limitations</strong><br><strong>Limitations of automated oversight:</strong>
The properties of wanted or unwanted model behavior often cannot be entirely specified, limiting the effectiveness of guardrails.</p><p><strong>Limitations of human oversight:</strong>
The downsides of human oversight aare:</p><ol><li>More costly and slower</li><li>The risk of &lsquo;approval fatigue&rsquo; where humans are overwhelmed by approval requests, especially if the large majority of those are okay.</li><li>Lack of expertise to judge</li><li>Lack of involvement in the situation to make the judgement - which is a form of lack of expertis</li></ol><p>Ad.4: Regarding lack of involvement: for human operators or drivers of automated systems like self-driving cars, staying actively involved or having a role in the control loop helps maintain situational awareness. This involvement can prevent complacency and ensures that the human operator is ready to take over control if the automated system fails or encounters a scenario it cannot handle. However, maintaining situational awareness can be challenging with high levels of automation due to the &ldquo;out-of-the-loop&rdquo; phenomenon, where the human operator may become disengaged from the task at hand, leading to slower response times or decreased effectiveness in managing unexpected situations.
In other words: If you as a user are not involved actively in performing a task, then you lose understanding of whether it is correct or what the impact can be. If you then only need to confirm something by saying &lsquo;go ahead&rsquo; or &lsquo;cancel&rsquo;, a badly informed &lsquo;go ahead&rsquo; is easy to pick.</p><p><strong>References</strong><br>Useful standards include:</p><ul><li>ISO/IEC 42001 B.9.3 defines controls for human oversight and decisions regarding autonomy. Gap: covers this control partly (human oversight only, not business logic)</li><li>Not covered further in ISO/IEC standards.</li></ul><h4>#LEAST MODEL PRIVILEGE<span class="absolute -mt-20" id=least-model-privilege></span>
<a href=#least-model-privilege class=subheading-anchor aria-label="Permalink for this section"></a></h4><blockquote><p>Category: runtime information security control<br>Permalink: <a href=https://owaspai.org/goto/leastmodelprivilege/ target=_blank rel=noopener>https://owaspai.org/goto/leastmodelprivilege/</a></p></blockquote><p><strong>Description</strong><br>Least model privilege: Minimize what a model can do (trigger actions or access data), to prevent harm in case the model is manipulated, or makes a mistake by itself.</p><p><strong>Implementation</strong></p><ul><li><strong>Limit both permissions and attack surface</strong>. Privileges can be controlled by confguring permissions in an authorization mechanism, and by removing access to elements and thus reducing the attack surface for a manipulated model (e.g., isolating or sand-boxing an agent by removing commands it could call from an image, or limit network access).</li><li><strong>Honor limitations of the served</strong>: Execute actions of AI systems with the rights and privileges of the user or service being served. This ensures that no actions are invoked and no data is retrieved outside authorizations. Note that the served is always just the initiator of an action - for example if the initiator wants the AI system to provide information to others. In that case, the authorization of those others should also be taken into account.</li><li><strong>Task-based minimization</strong>: Take the served-limitation a step further by reducing actions that the model can potentially trigger, and what they can be triggered on, to the minimum necessary for the reasonably foreseeable use cases. See below for the flexibility balance here. The purpose of this is <em>blast radius control</em>: to limit the attack surface in case the AI model is compromised, or in case the AI model makes a mistake. This requires mechanisms that may not be offered by the Identity and Access Management in place, such as: ephemeral tokens, dynamic permissions, and narrow permission control at scale, combined with trust establishment and potential revocation across different domains. See &lsquo;Strategies for task-based minmimization&rsquo; below.</li><li><strong>Avoid implementing authorization in Generative AI instructions</strong>, as these are vulnerable to hallucinations and manipulation (e.g., prompt injection). This is especially applicable in Agentic AI. This includes the prevention of Generative AI outputting commands that include references to the user context as it would open up the opportunity to escalate privileges by manipulating that output.</li></ul><p>Example case: an AI model is connected to an email facility to summarize incoming emails to an end user:</p><ul><li>Honor limitations of the actor: make sure the AI only can access the emails the end user can access.
-Task-based minimization: limit the email access to read-only - with the goal to avoid the model being manipulated to for example send spam emails, or include misinformation in the summaries, or gain access to sensitive emails of the user and send those to the attacker.</li></ul><p><strong>Flexibility balance</strong><br>How to strike the balance between:</p><ol><li>a general purpose AI agent that has all permissions which you can assign to anything, and</li><li>a large set of AI agents, each for a different type of task with the right set of permissions to prevent it stepping out of bounds?
Option 1 is the easiest extreme and option 2 requires more effort and also may cause certain workflows to fail because the agent didn&rsquo;t have permissions, causing user frustration and administrator effort to further tailor agents and permissions.<br>Still, least model privilege is critical if successful manipulation is probable and the potential effects are severe. The best practice is to at least have separate agents for the permissions that may have severe effects (e.g. execute run commands). This puts the responsibility of selecting the right permissions to the actor choosing the agent. This can introduce the risk of the actor (person or agent) choosing an agent with too many permissions because they are not sufficiently informed, or they prefer flexibility over security too much. If this risk is real, then dynamic minimization of permissions is required. This requires the implementation of logic that sets action permissions based on knowledge of the intent (e.g. an agent that is assigned to summarize a ticket only gets access to read tickets), and knowledge of potential risks (e.g. reducing permissions automatically the moment that untrusted input is introduced in an agent workflow).</li></ol><p>One of the most powerful things to let AI agents do is to execute code. That is where task-based minimization becomes a challenge because on the one hand you want to broaden the possibilities for the agents, and on the other hand you want to limit those possibilities for attackers. Solutions include:</p><ul><li>Replacing arbitrary code execution with the execution of a limited set of API calls</li><li>Removing commands (e.g. deleting them from a deployed operating system</li><li>Sand boxing the code execution by for example network segmentation, to minimize the attack surface of commands</li></ul><p><strong>Strategies for task-based minimization</strong><br>As mentioned above, it is essential to minimize actions that the model can potentially trigger, and what they can be triggered on. This needs to be minimized based on who or what is served (see above) and on the task. Strategies for task-based minimization include:</p><ul><li><strong>Harden based on general intent</strong>: Since agents and agentic systems typically don&rsquo;t have a single fixed task: at least minimize permissions based on the reasonably foreseeable use cases.</li><li><strong>Harden based on prompt intent</strong>: The orginal prompt to an agent contains intent. Mechanisms (typically LLM based) can interpret that and set permissions. This is where least privilege mechanisms start to overlap with what is presented under <a href=/goto/oversight/>#OVERSIGHT</a>, including grounding checks. The difference is that the least privilege mechanism uses preventative permissions and the oversight mechanism is reactive. The effect is the same, and the advantage of permissions can be that they may serve as permissions for any subagents, which allows for inheritance of the context in the agentic flow.</li><li><strong>Harden based on role assignment</strong>: As soon as an agent or agentic flow is assigned to a specific task (e.g., an LLM assigned to review new code in the form of a merge request), the permissions can be minimized to the role to perform that task.</li><li><strong>Harden based on risk elevation</strong>: Logic can be implemented to harden permissions the moment that certain input enters an agentic flow - from un untrusted agent, from an untrusted source (e.g., a public comment database), or the other way around: sensitive data entering the flow. From that moment, the logic can for example disable all actions that allow sending out sensitive data. This needs to be balanced of course with whether the intent is still possible, and whether the inclusion of those risk elevating elements is reason to downgrade agentic capability.</li><li><strong>Downgrading subagents</strong>: Have inter-agent calls include reduced permission sets, where possible.For example: an email handling agent calling another agent to summarize an email message. Such a hand-down mechanism is best performed outside the LLM because of reliability issues of the model. For example, <em>LangChain</em> supports this mechanism using tools and subagents. However, fine-grained runtime permission handoff (like delegated scoped credentials) is not native.</li><li><strong>Hardening as incident response</strong>: Based on the level of suspicion, automated or manual response mechanisms may harden an agentic flow, to reduce blast radius of a potentially corrupted state, without fully stopping it - so to limit interference of the response.</li><li><strong>Ephemeral permissions</strong>: if an assigned task is expected to be done in a certain amount of time, then certain permissions can be set as temporary, to prevent manipulated agents making use of these permissions to cause harm. This can be seen as <em>temporal blast radius control</em>.</li><li><strong>Informing and nudging users to harden agents</strong>: End users and administrators can have an important role in hardening permissions of agentic AI based on the task. Their general incentive is to NOT harden agents because 1) it takes time to analyse what is necessary, and 2) if the user is wrong, agentic tasks may fail. Therefore, it is important to create awareness with users about the risks and information on which permissions to set for which functions and which not to set for which risks. This is a form of <a href=/goto/aitransparency/>#AI TRANSPARENCY</a>. Strategies include: providing user training and user documentation on this subject, showing guidance in the user interface, with suggestions, and giving warnings in case riskful permissions are set.</li></ul><p><strong>References</strong></p><ul><li>ISO 27002 control 8.2 Privileged access rights. Gap: covers this control fully, with the particularity that privileges assigned to autonomous model decisions need to be assigned with the risk of unwanted model behaviour in mind.</li><li><a href=https://www.opencre.org/cre/368-633 target=_blank rel=noopener>OpenCRE on least privilege</a> Gap: idem</li><li><a href=https://arxiv.org/abs/2505.19301 target=_blank rel=noopener>A Novel Zero-Trust Identity Framework for Agentic AI: Decentralized Authentication and Fine-Grained Access Control</a></li></ul><h4>#MODEL ALIGNMENT<span class="absolute -mt-20" id=model-alignment></span>
<a href=#model-alignment class=subheading-anchor aria-label="Permalink for this section"></a></h4><blockquote><p>Category: development-time and runtime AI engineer control<br>Permalink: <a href=https://owaspai.org/goto/modelalignment/ target=_blank rel=noopener>https://owaspai.org/goto/modelalignment/</a></p></blockquote><p><strong>Description and objective</strong><br>In the context of Generative AI (e.g., LLMs), alignment refers to the process of ensuring that the model&rsquo;s behavior and outputs are consistent with human values, intentions, and ethical standards.</p><p>Controls external to the model to manage model behaviour are:</p><ul><li><a href=/goto/oversight/>OVERSIGHT</a>: conventional mechanisms responding to the actual outcome of the model</li><li><a href=/goto/leastmodelprivilege/>LEAST MODEL PRIVILEGE</a>: conventional mechanisms that put boundaries on what the model can affect</li><li><a href=/goto/promptinjectioniohandling/>PROMPT INJECTION I/O handling</a>: detection mechanisms on input and output to prevent unwanted behaviour</li></ul><p>The intent of Model alignment is achieve similar goals by baking it into the model itself, through training and instruction.</p><p><strong>Implementation</strong><br>Achieving the goal of model alignment involves multiple layers:</p><ol><li><p>Training-Time Alignment: the maker of the model shaping its core behaviour</p><p>This is often what people mean by &ldquo;model alignment&rdquo; in the strict sense:</p><ul><li>Training data choices</li><li>Fine-tuning (on aligned examples: helpful, harmless, honest)</li><li>Reinforcement learning from human feedback (RLHF) or other reward modeling</li></ul></li><li><p>Deployment-Time Alignment (Including System Prompts)</p><p>Even if the model is aligned during training, its actual behavior during use is also influenced by:</p><ul><li>System prompts / instruction prompts</li><li>Guardrails built into the AI system and external tools that oversee or control responses (like content filters or output constraints) - see the external controls mentioned above</li></ul></li></ol><p>See <a href=/goto/culturesensitivealignment/>the appendix on culture-sensitive alignment</a>.</p><p><strong>Limitations</strong><br>Advantage of Model alignment over the external mechanisms:</p><ul><li>Training-time alignment is in essence able to capture complex behavioural boundaries in the form of many examples of wanted and less-wanted behaviour</li><li>Recognition of unwanted behaviour is very flexible as the GenAI model typically has powerful judgement abilities.</li></ul><p>Disadvantages of Model alignment:</p><ul><li>A model&rsquo;s ability to behave through alignment suffers from reliability issues, as it can be prone to manipulation or imperfect memorization and application of what it has learned and what it has been told.</li><li>The boundaries of unwanted model behaviour may change after model training (e.g., through new findings), forcing the use of system prompts and/or external controls</li></ul><p>Therefore, alignment should be seen as a probabilistic, model-internal control that must be combined with deterministic, external mechanisms for high-risk or regulated use cases.</p><h4>#AI TRANSPARENCY<span class="absolute -mt-20" id=ai-transparency></span>
<a href=#ai-transparency class=subheading-anchor aria-label="Permalink for this section"></a></h4><blockquote><p>Category: governance and runtime control<br>Permalink: <a href=https://owaspai.org/goto/aitransparency/ target=_blank rel=noopener>https://owaspai.org/goto/aitransparency/</a></p></blockquote><p><strong>Description</strong><br>AI transparency: Informing users on the AI system&rsquo;s properties to enable them to adjust how they rely on it, what data they are willing to send to it, and what additional mitigations to apply. These AI system properties can include:</p><ul><li>Rough working of the model</li><li>The training approach</li><li>Type of data used and the source</li><li>Expected accuracy and robustness of the AI system&rsquo;s output</li><li>Any residual (security) risks</li></ul><p>Note that transparency here is about providing abstract information regarding the AI system and is therefore something else than <em>explainability</em> of model decisions. The simplest form of transparencey is to inform users that an AI model is being involved. This is for example required by the EU AI Act for chatbots.</p><p>See the <a href=#discrete>DISCRETE</a> control for the balance between being transparent and being discrete about the model.</p><p>Example: Informing users that when they choose an agent to perform a task, that the agent could be manipulated if it reads untrusted data and what consequences that could have (residual security risk) - followed by a recommendation to configure the permissions of the agent to the minimal set for the task.</p><p><strong>References</strong></p><ul><li>ISO/IEC 42001 B.7.2 describes data management to support transparency. Gap: covers this control minimally, as it only covers the data management part.</li><li>Not covered further in ISO/IEC standards.</li><li><a href=https://llmtop10.com/llm09/ target=_blank rel=noopener>OWASP top 10 for LLM 09 on over-reliance</a></li></ul><h4>#CONTINUOUS VALIDATION<span class="absolute -mt-20" id=continuous-validation></span>
<a href=#continuous-validation class=subheading-anchor aria-label="Permalink for this section"></a></h4><blockquote><p>Category: development-time and runtime AI engineer control<br>Permalink: <a href=https://owaspai.org/goto/continuousvalidation/ target=_blank rel=noopener>https://owaspai.org/goto/continuousvalidation/</a></p></blockquote><p><strong>Description</strong><br>Continuous validation: by frequently testing the behaviour of the model against an appropriate test set, it is possible to detect sudden changes caused by a permanent attack (e.g. data poisoning, model poisoning), and also some robustness issues against for example evasion attacks.</p><p>Continuous validation is a process that is often in place to detect other issues than attacks: system failures, or the model performance going down because of changes in the real world since it was trained (model drift, model staleness). There are many performance metrics available and the best ones are those that align with the goal. These metrics pertain to correctness, but can also link to other aspects such as unwanted bias towards protected attributes.</p><p>Note that continuous validation is typically not suitable for detecting backdoor poisoning attacks, as these are designed to trigger with very specific input that would normally not be present in test sets. In fact, such attacks are often designed to pass validation tests.</p><p><strong>Objective</strong><br>Continuous validation helps verify that the model continues to behave as intended over time meeting acceptance criteria. In addition to supporting functional correctness, it provides a mechanism to detect unexpected or unexplained changes in model behaviour that may indicate permanent manipulation, such as data poisoning or model poisoning. Continuous validation may also surface certain robustness weaknesses, including limited exposure to evasion-related failure modes.
In some systems, model behaviour directly implements security-relevant functions, such as access control or policy enforcement, making correctness validation important from a cybersecurity perspective.</p><p><strong>Applicability</strong><br>Continuous validation applies to AI systems where changes in model behaviour could introduce security, safety, or compliance risks. It is particularly relevant when risks related to data poisoning, model poisoning, or unintended behavioural drift are not fully acceptable.</p><p><strong>Implementation</strong></p><p><strong>Implementation of timing and triggers</strong><br>Continuous validation can be performed at points in the system lifecycle where model behaviour may reasonably change or be at risk of manipulation. This includes:</p><ul><li>after initial training, retraining, or fine-tuning,</li><li>before deployment or redeployment, and</li><li>periodically during operation when the residual risk of model integrity is not considered acceptable.</li></ul><p>Operational validation is particularly relevant when models remain exposed to updates, external dependencies, or environments where unauthorized modification is plausible. The frequency and scope of validation are typically informed by risk analysis and the criticality of the model’s output.</p><p><strong>Implementation of degradation detection and response handling</strong><br>Validation results can be monitored for unexpected or unexplained changes in model performance, which may indicate permanent behavioural changes caused by attacks, configuration errors, or environmental drift.<br>When performance degradation or abnormal behaviour is observed, possible response options include:</p><ul><li>investigating the underlying cause;</li><li>continuing operation when degradation is temporary and within acceptable bounds;</li><li>rolling back to a previous model version with known behaviour;</li><li>restricting usage to lower-risk scenarios or specific tasks;</li><li>introducing additional human or automated oversight for high-risk outputs to limit error propagation; or</li><li>temporarily disabling the system if continued operation is unsafe.<br>The choice of response influences both the impact of the issue and the timeliness of recovery.</li></ul><p><strong>Implementation of test data management and protection</strong><br>Test datasets serve as a reference for intended or acceptable model behaviour and therefore benefit from protection against manipulation. Storing test data separately from training data or model artifacts can reduce the likelihood that attackers influence both the model and its evaluation baseline.
When test data remains less exposed than training data or deployed model components, continuous validation can help surface integrity issues even if other parts of the system are compromised.</p><p><strong>Risk-Reduction Guidance</strong><br>Continuous validation can be an effective mechanism for detecting permanent behavioural changes caused by attacks such as data poisoning or model poisoning. Detection timeliness depends on how frequently validation is performed and whether the manipulated model has already been deployed.
The level of impact from a detected degradation depends on both the severity of the behaviour change and the response taken. Responses may include investigation, rollback to a previous model version, restricting usage to lower-risk scenarios, or introducing additional oversight for high-risk outputs.
Continuous validation is not a strong countermeasure against evasion attacks and does not guarantee detection of attacks designed to bypass validation, such as trigger-based backdoor poisoning.
For poisoning introduced during development or training, validation before deployment can prevent exposure entirely, whereas poisoning introduced during operation may only be detected after some period of use, depending on validation frequency.</p><p><strong>Particularity</strong><br>There is a terminology difference between AI performance testing and traditional performance testing in non-AI systems. The latter focuses on efficiency metrics such as latency or throughput, whereas performance testing of AI models focuses on behavioural correctness, robustness, and consistency with intended use. It may also include checks for bias or unintended decision patterns.</p><p><strong>Limitations</strong><br>Continuous validation relies on the representativeness and integrity of the test dataset. Attacks that are triggered only by rare or highly specific inputs may not be detected if those inputs are absent from test sets.
If attackers are able to manipulate both the model and the test data, validation results may no longer be trustworthy. Validation alone therefore does not replace other integrity and monitoring controls.</p><p><strong>References</strong>
Useful standards include:</p><ul><li>ISO 5338 (AI lifecycle) Continuous validation. Gap: covers this control fully</li><li>ISO/IEC 24029-2:2023 Artificial intelligence (AI) — Assessment of the robustness of neural networks</li><li>ISO/IEC 24027:2021 Bias in AI systems and datasets</li><li>ISO/IEC 25059:2023 Software engineering — Systems and software Quality Requirements and Evaluation (SQuaRE) — Quality model for AI systems</li><li>CEN/CLC JT021008 AI trustworthiness framework</li></ul><h4>#EXPLAINABILITY<span class="absolute -mt-20" id=explainability></span>
<a href=#explainability class=subheading-anchor aria-label="Permalink for this section"></a></h4><blockquote><p>Category: runtime AI engineer control<br>Permalink: <a href=https://owaspai.org/goto/explainability/ target=_blank rel=noopener>https://owaspai.org/goto/explainability/</a></p></blockquote><p><strong>Description</strong><br>Explainability: Explaining how individual model decisions are made, a field referred to as Explainable AI (XAI), can aid in gaining user trust in the model. In some cases, this can also prevent overreliance, for example, when the user observes the simplicity of the &lsquo;reasoning&rsquo; or even errors in that process. See <a href=https://hai.stanford.edu/news/ai-overreliance-problem-are-explanations-solution target=_blank rel=noopener>this Stanford article on explainability and overreliance</a>. Explanations of how a model works can also aid security assessors to evaluate AI security risks of a model.</p><h4>#UNWANTED BIAS TESTING<span class="absolute -mt-20" id=unwanted-bias-testing></span>
<a href=#unwanted-bias-testing class=subheading-anchor aria-label="Permalink for this section"></a></h4><blockquote><p>Category: development-time and runtime AI engineer control<br>Permalink: <a href=https://owaspai.org/goto/unwantedbiastesting/ target=_blank rel=noopener>https://owaspai.org/goto/unwantedbiastesting/</a></p></blockquote><p><strong>Description</strong><br>Unwanted bias testing: By doing test runs of the model to measure unwanted bias, unwanted behaviour caused by an attack can be detected. The details of bias detection fall outside the scope of this document as it is not a security concern - other than that, an attack on model behaviour can cause bias.</p></div></div></main><aside class="hidden md:block docs-toc docs-sidebar-column"><div class="toc-container sticky"><h3 class="toc-title py-2">On this page</h3><div class="toc-content bg-gradient-to-b from-[#CCF6CE] to-[#FDFBFB] border border-gray-300 rounded-lg p-4"><nav id=TableOfContents><ul><li><a href=#11-general-governance-controls>1.1 General governance controls</a><ul><li><ul><li><a href=#ai-program>#AI PROGRAM</a></li><li><a href=#sec-program>#SEC PROGRAM</a></li><li><a href=#sec-dev-program>#SEC DEV PROGRAM</a></li><li><a href=#dev-program>#DEV PROGRAM</a></li><li><a href=#check-compliance>#CHECK COMPLIANCE</a></li><li><a href=#sec-educate>#SEC EDUCATE</a></li></ul></li></ul></li><li><a href=#12-general-controls-for-sensitive-data-limitation>1.2 General controls for sensitive data limitation</a><ul><li><ul><li><a href=#data-minimize>#DATA MINIMIZE</a></li><li><a href=#allowed-data>#ALLOWED DATA</a></li><li><a href=#short-retain>#SHORT RETAIN</a></li><li><a href=#obfuscate-training-data>#OBFUSCATE TRAINING DATA</a></li><li><a href=#discrete>#DISCRETE</a></li></ul></li></ul></li><li><a href=#13-controls-to-limit-the-effects-of-unwanted-behaviour>1.3. Controls to limit the effects of unwanted behaviour</a><ul><li><ul><li><a href=#oversight>#OVERSIGHT</a></li><li><a href=#least-model-privilege>#LEAST MODEL PRIVILEGE</a></li><li><a href=#model-alignment>#MODEL ALIGNMENT</a></li><li><a href=#ai-transparency>#AI TRANSPARENCY</a></li><li><a href=#continuous-validation>#CONTINUOUS VALIDATION</a></li><li><a href=#explainability>#EXPLAINABILITY</a></li><li><a href=#unwanted-bias-testing>#UNWANTED BIAS TESTING</a></li></ul></li></ul></li></ul></nav></div></div></aside></div><section id=contact-form class="relative bg-[#EDF7ED] py-16"><div class="max-w-[1400px] mx-auto px-6 md:px-12"><h2 class="text-3xl md:text-4xl font-bold text-center text-[#1a1a2e] mb-16">We are always happy to assist you!</h2><div class="grid grid-cols-1 md:grid-cols-2 gap-12 items-start"><div><div class="flex flex-wrap gap-4 mb-8"><a href=https://github.com/OWASP/www-project-ai-security-and-privacy-guide/discussions target=_blank class="w-10 h-10 bg-white rounded-md flex items-center justify-center shadow"><i class="fab fa-github text-gray-700"></i></a>
<a href=https://owasp.slack.com/join/shared_invite/zt-3iwx0wk01-vLwSQy1NfkD0Cv5UJeMbwQ#/shared-invite/email target=_blank class="w-10 h-10 bg-white rounded-md flex items-center justify-center shadow"><img src=/images/slack.png alt=Slack class="w-6 h-6"></a>
<a href=https://www.linkedin.com/company/owasp-ai-exchange/ target=_blank class="w-10 h-10 bg-white rounded-md flex items-center justify-center shadow"><i class="fab fa-linkedin-in text-blue-700"></i></a>
<a href="https://www.youtube.com/playlist?list=PLCZNSZ1gyRoD5droM_qyXYyBofj00x_zO" target=_blank class="w-10 h-10 bg-white rounded-md flex items-center justify-center shadow"><i class="fab fa-youtube text-red-600"></i></a></div><h3 class="text-2xl font-bold text-[#1a1a2e] mb-4">Send us a message</h3><p class="text-gray-500 leading-relaxed">Have questions about AI security? Want to contribute to our mission?
We'd love to hear from you. Reach out through any of our channels or
use the contact form.</p></div><div><div id=form-message class="hidden mb-4 p-4 rounded-lg"></div><div class="block md:hidden bg-white shadow-lg rounded-xl p-6"><form id=contact-form-mobile class="flex flex-col gap-6" novalidate><div class="grid grid-cols-1 gap-4"><div><input id=mobile-first-name name=first_name type=text placeholder="First Name*" required class="p-4 border border-gray-300 rounded-lg text-base w-full focus:outline-none focus:ring-2 focus:ring-green-500">
<span class="error-message hidden text-red-600 text-sm mt-1"></span></div><div><input id=mobile-last-name name=last_name type=text placeholder="Last Name*" required class="p-4 border border-gray-300 rounded-lg text-base w-full focus:outline-none focus:ring-2 focus:ring-green-500">
<span class="error-message hidden text-red-600 text-sm mt-1"></span></div></div><div><input id=mobile-email name=email type=email placeholder="Email Address*" required class="p-4 border border-gray-300 rounded-lg text-base w-full focus:outline-none focus:ring-2 focus:ring-green-500">
<span class="error-message hidden text-red-600 text-sm mt-1"></span></div><div><textarea id=mobile-message name=message placeholder=Message rows=4 class="p-4 border border-gray-300 rounded-lg text-base resize-vertical w-full focus:outline-none focus:ring-2 focus:ring-green-500"></textarea>
<span class="error-message hidden text-red-600 text-sm mt-1"></span></div><button type=submit id=mobile-submit-btn class="bg-green-500 text-white px-6 py-3 rounded-lg font-bold flex items-center justify-center gap-2 w-max hover:bg-green-600 transition disabled:opacity-50 disabled:cursor-not-allowed">
<span id=mobile-submit-text>Submit</span>
<span id=mobile-submit-arrow>→</span>
<span id=mobile-submit-loader class=hidden>⏳</span></button></form></div><form id=contact-form-desktop class="hidden md:flex flex-col gap-6" novalidate><div class="grid grid-cols-1 md:grid-cols-2 gap-4"><div><input id=desktop-first-name name=first_name type=text placeholder="First Name*" required class="p-4 border border-gray-300 rounded-lg text-base w-full focus:outline-none focus:ring-2 focus:ring-green-500">
<span class="error-message hidden text-red-600 text-sm mt-1"></span></div><div><input id=desktop-last-name name=last_name type=text placeholder="Last Name*" required class="p-4 border border-gray-300 rounded-lg text-base w-full focus:outline-none focus:ring-2 focus:ring-green-500">
<span class="error-message hidden text-red-600 text-sm mt-1"></span></div></div><div><input id=desktop-email name=email type=email placeholder="Email Address*" required class="p-4 border border-gray-300 rounded-lg text-base w-full focus:outline-none focus:ring-2 focus:ring-green-500">
<span class="error-message hidden text-red-600 text-sm mt-1"></span></div><div><textarea id=desktop-message name=message placeholder=Message rows=4 class="p-4 border border-gray-300 rounded-lg text-base resize-vertical w-full focus:outline-none focus:ring-2 focus:ring-green-500"></textarea>
<span class="error-message hidden text-red-600 text-sm mt-1"></span></div><button type=submit id=desktop-submit-btn class="bg-green-500 text-white px-6 py-3 rounded-lg font-bold flex items-center justify-center gap-2 w-max hover:bg-green-600 transition disabled:opacity-50 disabled:cursor-not-allowed">
<span id=desktop-submit-text>Submit</span>
<span id=desktop-submit-arrow>→</span>
<span id=desktop-submit-loader class=hidden>⏳</span></button></form></div></div></div></section><script>(function(){const r="https://usabilitydesigns.com/calendar/send-mail.php",t=document.getElementById("contact-form-mobile"),n=document.getElementById("contact-form-desktop"),e=document.getElementById("form-message");function s(e){const t=e.value.trim(),i=e.name;let n=!0,o="";e.classList.remove("border-red-500");const s=e.parentElement.querySelector(".error-message");if(s&&(s.classList.add("hidden"),s.textContent=""),e.hasAttribute("required")&&!t&&(n=!1,o="This field is required"),i==="email"&&t){const e=/^[^\s@]+@[^\s@]+\.[^\s@]+$/;e.test(t)||(n=!1,o="Please enter a valid email address")}return i==="first_name"&&t&&t.length<2&&(n=!1,o="First name must be at least 2 characters"),i==="last_name"&&t&&t.length<2&&(n=!1,o="Last name must be at least 2 characters"),!n&&s&&(e.classList.add("border-red-500"),s.textContent=o,s.classList.remove("hidden")),n}function c(e){const o=e.querySelectorAll("input[required], textarea[required]");let t=!0;o.forEach(e=>{s(e)||(t=!1)});const n=e.querySelector('input[name="email"]');return n&&n.value.trim()&&(s(n)||(t=!1)),t}function o(t,n=!1){e.textContent=t,e.className=n?"mb-4 p-4 rounded-lg bg-red-100 border border-red-400 text-red-700":"mb-4 p-4 rounded-lg bg-green-100 border border-green-400 text-green-700",e.classList.remove("hidden"),e.scrollIntoView({behavior:"smooth",block:"nearest"}),n||setTimeout(()=>{e.classList.add("hidden")},5e3)}function i(e,t){const n=e.querySelector('button[type="submit"]'),s=e.querySelector('[id$="-submit-text"]'),o=e.querySelector('[id$="-submit-arrow"]'),i=e.querySelector('[id$="-submit-loader"]');n&&(n.disabled=t),s&&(s.style.display=t?"none":"inline"),o&&(o.style.display=t?"none":"inline"),i&&(i.style.display=t?"inline":"none")}function l(e){e.reset(),e.querySelectorAll(".error-message").forEach(e=>{e.classList.add("hidden"),e.textContent=""}),e.querySelectorAll("input, textarea").forEach(e=>{e.classList.remove("border-red-500")})}async function a(t,n){if(n.preventDefault(),e.classList.add("hidden"),!c(t)){o("Please fill in all required fields correctly.",!0);return}const s=new FormData(t),a={first_name:s.get("first_name"),last_name:s.get("last_name"),email:s.get("email"),message:s.get("message")||""};i(t,!0);try{const n=await fetch(r,{method:"POST",headers:{"Content-Type":"application/json"},body:JSON.stringify(a)});let e;const s=n.headers.get("content-type");if(s&&s.includes("application/json"))e=await n.json();else{const t=await n.text();try{e=JSON.parse(t)}catch{n.ok?e={success:!0}:e={success:!1,error:"Server error occurred"}}}e.success?(o("Thank you! Your message has been sent successfully.",!1),l(t)):o(e.error||"Failed to send message. Please try again later.",!0)}catch(e){console.error("Form submission error:",e),o("An error occurred while sending your message. Please try again later.",!0)}finally{i(t,!1)}}t&&(t.addEventListener("submit",e=>a(t,e)),t.querySelectorAll("input, textarea").forEach(e=>{e.addEventListener("blur",()=>s(e))})),n&&(n.addEventListener("submit",e=>a(n,e)),n.querySelectorAll("input, textarea").forEach(e=>{e.addEventListener("blur",()=>s(e))}))})()</script></main><script src=https://cdn.tailwindcss.com></script>
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.6.0/css/all.min.css><link href="https://fonts.googleapis.com/css2?family=Inter:wght@500&family=Manrope:wght@600&family=Roboto:wght@500&display=swap" rel=stylesheet><script>tailwind.config={theme:{extend:{fontFamily:{inter:["Inter","sans-serif"],manrope:["Manrope","sans-serif"],roboto:["Roboto","sans-serif"]}}}}</script><footer class="bg-black text-white w-full"><div class="container mx-auto px-4 md:px-10 md:py-4"><div class=md:hidden><div class="text-center mb-4"><a href=/ class=inline-block><img src=/images/Owasp-AI-Exchange-Logo.png alt="AI Exchange" class="h-10 md:h-10 w-auto mx-auto mb-4 filter brightness-0 invert"></a><p class="text-sm opacity-90 max-w-md mx-auto">Advancing AI security through community collaboration and open-source resources.</p></div><div class="text-center mb-8"><ul class=space-y-3><li><a href=/ class="block text-base hover:text-gray-300 transition">Home</a></li><li><a href=/docs/ai_security_overview/ class="block text-base hover:text-gray-300 transition">Overview</a></li><li><a href=/media/ class="block text-base hover:text-gray-300 transition">Media</a></li><li><a href=/sponsor/ class="block text-base hover:text-gray-300 transition">Sponsor</a></li><li><a href=/contribute/ class="block text-base hover:text-gray-300 transition">Contribute</a></li><li><a href=/connect/ class="block text-base hover:text-gray-300 transition">Connect</a></li></ul></div><div class="border-t border-gray-600 mb-6"></div><div class="flex justify-center space-x-4 mb-6"><a href=https://github.com/OWASP/www-project-ai-security-and-privacy-guide target=_blank class="w-12 h-12 flex items-center justify-center border border-white rounded-full hover:bg-gray-800 transition"><i class="fab fa-github text-lg"></i></a>
<a href=https://owasp.slack.com/join/shared_invite/zt-3iwx0wk01-vLwSQy1NfkD0Cv5UJeMbwQ#/shared-invite/email target=_blank class="w-12 h-12 flex items-center justify-center border border-white rounded-full hover:bg-gray-800 transition"><i class="fab fa-slack text-lg"></i></a>
<a href=https://www.linkedin.com/company/owasp-ai-exchange/ target=_blank class="w-12 h-12 flex items-center justify-center border border-white rounded-full hover:bg-gray-800 transition"><i class="fab fa-linkedin-in text-lg"></i></a>
<a href="https://www.youtube.com/playlist?list=PLCZNSZ1gyRoD5droM_qyXYyBofj00x_zO" target=_blank class="w-12 h-12 flex items-center justify-center border border-white rounded-full hover:bg-gray-800 transition"><i class="fab fa-youtube text-lg"></i></a></div><div class="border-t border-gray-600 mb-6"></div><div class="text-center mb-6"><p class=text-sm>Copyright © owasp.org 2026. All Rights Reserved.</p></div><div class=text-center><p class="text-sm mb-2">Designed & Developed by:</p><div class="flex items-center justify-center space-x-2"><img src=/images/usability.png alt="Usability Icon" class="h-6 w-auto">
<a href=https://usabilitydesigns.com/ class="text-sm font-medium text-yellow-400 hover:text-yellow-300 transition">USABILITY DESIGNS</a></div></div></div><div class="hidden md:block py-4"><div class="flex justify-between items-start"><div class="flex flex-col logo-section"><a href=/ class=logo-link><img src=/images/Owasp-AI-Exchange-Logo.png alt="OWASP AI Exchange" class="site-logo h-[36px] w-auto"></a><ul class="flex space-x-6 mt-3 font-roboto text-[15px]"><li><a href=/ class=hover:text-white>Home</a></li><li><a href=/docs/ai_security_overview/ class=hover:text-white>Overview</a></li><li><a href=/media/ class=hover:text-white>Media</a></li><li><a href=/sponsor/ class=hover:text-white>Sponsor</a></li><li><a href=/contribute/ class=hover:text-white>Contribute</a></li><li><a href=/connect/ class=hover:text-white>Connect</a></li></ul></div><div class="flex space-x-3 mt-2"><a href=https://github.com/OWASP/www-project-ai-security-and-privacy-guide target=_blank class="w-8 h-8 flex items-center justify-center border border-white rounded-full hover:bg-gray-800 transition"><i class="fab fa-github"></i></a>
<a href=https://owasp.slack.com/join/shared_invite/zt-3iwx0wk01-vLwSQy1NfkD0Cv5UJeMbwQ#/shared-invite/email target=_blank class="w-8 h-8 flex items-center justify-center border border-white rounded-full hover:bg-gray-800 transition"><i class="fab fa-slack"></i></a>
<a href=https://www.linkedin.com/company/owasp-ai-exchange/ target=_blank class="w-8 h-8 flex items-center justify-center border border-white rounded-full hover:bg-gray-800 transition"><i class="fab fa-linkedin-in"></i></a>
<a href="https://www.youtube.com/playlist?list=PLCZNSZ1gyRoD5droM_qyXYyBofj00x_zO" target=_blank class="w-8 h-8 flex items-center justify-center border border-white rounded-full hover:bg-gray-800 transition"><i class="fab fa-youtube"></i></a></div></div><div class="border-t border-gray-600 mt-3"></div><div class="flex w-full flex-row justify-between items-center pt-3"><p class="font-roboto text-[14px]">Copyright © owasp.org 2026. All Rights Reserved.</p><p class="flex gap-2 text-[12px] items-center">PROUDLY DESIGNED AND DEVELOPED BY:
<img src=/images/usability.png alt="Usability Icon" class="h-4 inline-block">
<a href=https://usabilitydesigns.com/ class=text-[#FFAE3C]>USABILITY DESIGNS</a></p></div></div></div></footer></body></html>